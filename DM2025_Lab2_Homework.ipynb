{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rix1SK0S_sZO"
      },
      "source": [
        "### **Student Information**\n",
        "Name: ÊûóÊ¨£Ë´≠\n",
        "\n",
        "Student ID: 114065543\n",
        "\n",
        "GitHub ID: hsinyulin1323@gmail.com\n",
        "\n",
        "Kaggle name: Hsinyu Lin1323\n",
        "\n",
        "Kaggle private scoreboard snapshot:![Êà™Âúñ 2025-12-03 ‰∏ãÂçà6.23.07.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAB9gAAACUCAYAAADYgfJ5AAAMTmlDQ1BJQ0MgUHJvZmlsZQAASImVVwdYU8kWnltSSQgQiICU0JsgIiWAlBBaAOlFEJWQBAglxoSgYkcXFVy7iGBFV0EU2wrIYkNddWVR7K5lsaCysi4W7MqbEECXfeV7831z57//nPnnnHNn7r0DAKNDIJPloloA5Enz5bEhAewJySlsUhegAE1ABzTgLRAqZNzo6AgAy2D79/LmOkBU7RVHldY/+/9r0RaJFUIAkGiI00UKYR7EPwKANwtl8nwAiDLIW0zPl6nwWoh15dBBiKtVOFONm1U4XY0v9dvEx/IgfgQAmSYQyDMB0OyBPLtAmAl1GDBa4CwVSaQQ+0Psm5c3VQTxfIhtoQ2ck6HS56R/o5P5N830IU2BIHMIq2PpL+RAiUKWK5j5f6bjf5e8XOXgHDaw0rLkobGqmGHeHuVMDVdhGsTvpOmRURDrAIDiElG/vQqzspShCWp71Fao4MGcARbE4xS5cfwBPlYkCAyH2AjiDGluZMSATVGGJFhlA/OHlkvy+fEQ60NcLVYExQ3YnJBPjR2c93qGnMcd4J8K5P0+qPS/KHMSuGp9TCdLzB/Qx5wKs+KTIKZCHFggSYyEWBPiSEVOXPiATWphFi9y0EaujFXFYgmxXCwNCVDrY2UZ8uDYAfvdeYrB2LETWRJ+5AC+nJ8VH6rOFfZIKOj3H8aC9Yil3IRBHbFiQsRgLCJxYJA6dpwslibEqXlcX5YfEKsei9vLcqMH7PEAcW6IijeHOF5REDc4tiAfLk61Pl4sy4+OV/uJV2QLwqLV/uD7QQTggUDABkpY08FUkA0kbd0N3fBO3RMMBEAOMoEYOA4wgyOS+nuk8BoHCsGfEImBYmhcQH+vGBRA/vMwVsVJhjj11RFkDPSpVHLAY4jzQDjIhffKfiXpkAeJ4BFkJP/wSACrEMaQC6uq/9/zg+xXhguZiAFGOTgjmzFoSQwiBhJDicFEO9wQ98W98Qh49YfVBefgnoNxfLUnPCa0Ex4QrhE6CLemSIrkw7wcDzqgfvBAftK/zQ9uDTXd8ADcB6pDZZyFGwJH3BXOw8X94MxukOUN+K3KCnuY9t8i+OYJDdhRnCkoZQTFn2I7fKSmvabbkIoq19/mR+1r+lC+eUM9w+fnfZN9EWzDh1tiS7BD2FnsJHYea8YaABs7jjVirdhRFR5acY/6V9zgbLH9/uRAneFr5uuTVWVS4Vzr3OX8Sd2XL56Rr9qMvKmymXJJZlY+mwu/GGI2Xyp0GsV2cXZxA0D1/VG/3l7F9H9XEFbrV27h7wD4HO/r6/vpKxd2HIADHvCVcOQrZ8uBnxYNAM4dESrlBWoOV10I8M3BgLvPAJgAC2AL43EB7sAb+IMgEAaiQDxIBpOh91lwncvBdDAbLADFoBSsBOtABdgCtoNqsBccBA2gGZwEP4ML4BK4Bm7D1dMJnoEe8AZ8RBCEhNARJmKAmCJWiAPignAQXyQIiUBikWQkDclEpIgSmY0sREqR1UgFsg2pQQ4gR5CTyHmkHbmF3Ee6kJfIBxRDaaguaoxao6NRDspFw9F4dBKaiU5DC9FF6HK0HK1C96D16En0AnoN7UCfob0YwDQwFmaGOWIcjIdFYSlYBibH5mIlWBlWhdVhTfA5X8E6sG7sPU7EmTgbd4QrOBRPwIX4NHwuvgyvwKvxevw0fgW/j/fgXwh0ghHBgeBF4BMmEDIJ0wnFhDLCTsJhwhm4lzoJb4hEIotoQ/SAezGZmE2cRVxG3ETcRzxBbCc+JPaSSCQDkgPJhxRFEpDyScWkDaQ9pOOky6RO0juyBtmU7EIOJqeQpeQichl5N/kY+TL5CfkjRYtiRfGiRFFElJmUFZQdlCbKRUon5SNVm2pD9aHGU7OpC6jl1DrqGeod6isNDQ1zDU+NGA2JxnyNco39Guc07mu8p+nQ7Gk8WipNSVtO20U7QbtFe0Wn063p/vQUej59Ob2Gfop+j/5Ok6nppMnXFGnO06zUrNe8rPmcQWFYMbiMyYxCRhnjEOMio1uLomWtxdMSaM3VqtQ6onVDq1ebqT1GO0o7T3uZ9m7t89pPdUg61jpBOiKdRTrbdU7pPGRiTAsmjylkLmTuYJ5hduoSdW10+brZuqW6e3XbdHv0dPRc9RL1ZuhV6h3V62BhLGsWn5XLWsE6yLrO+jDCeAR3hHjE0hF1Iy6PeKs/Ut9fX6xfor9P/5r+BwO2QZBBjsEqgwaDu4a4ob1hjOF0w82GZwy7R+qO9B4pHFky8uDI34xQI3ujWKNZRtuNWo16jU2MQ4xlxhuMTxl3m7BM/E2yTdaaHDPpMmWa+ppKTNeaHjf9g63H5rJz2eXs0+weMyOzUDOl2TazNrOP5jbmCeZF5vvM71pQLTgWGRZrLVoseixNLcdbzrastfzNimLFscqyWm911uqttY11kvVi6wbrpzb6NnybQptamzu2dFs/22m2VbZX7Yh2HLscu012l+xRezf7LPtK+4sOqIO7g8Rhk0P7KMIoz1HSUVWjbjjSHLmOBY61jvedWE4RTkVODU7PR1uOThm9avTZ0V+c3ZxznXc43x6jMyZsTNGYpjEvXexdhC6VLlfH0scGj503tnHsC1cHV7HrZtebbky38W6L3VrcPrt7uMvd69y7PCw90jw2etzg6HKiOcs45zwJngGe8zybPd97uXvlex30+svb0TvHe7f303E248Tjdox76GPuI/DZ5tPhy/ZN893q2+Fn5ifwq/J74G/hL/Lf6f+Ea8fN5u7hPg9wDpAHHA54y/PizeGdCMQCQwJLAtuCdIISgiqC7gWbB2cG1wb3hLiFzAo5EUoIDQ9dFXqDb8wX8mv4PWEeYXPCTofTwuPCK8IfRNhHyCOaxqPjw8avGX8n0ipSGtkQBaL4UWui7kbbRE+L/imGGBMdUxnzOHZM7OzYs3HMuClxu+PexAfEr4i/nWCboExoSWQkpibWJL5NCkxandQxYfSEORMuJBsmS5IbU0gpiSk7U3onBk1cN7Ez1S21OPX6JJtJMyadn2w4OXfy0SmMKYIph9IIaUlpu9M+CaIEVYLedH76xvQeIU+4XvhM5C9aK+oS+4hXi59k+GSsznia6ZO5JrMryy+rLKtbwpNUSF5kh2ZvyX6bE5WzK6cvNyl3Xx45Ly3viFRHmiM9PdVk6oyp7TIHWbGsY5rXtHXTeuTh8p0KRDFJ0ZivC3/0W5W2yu+U9wt8CyoL3k1PnH5ohvYM6YzWmfYzl858Uhhc+MMsfJZwVstss9kLZt+fw52zbS4yN31uyzyLeYvmdc4PmV+9gLogZ8GvRc5Fq4teL0xa2LTIeNH8RQ+/C/mutlizWF58Y7H34i1L8CWSJW1Lxy7dsPRLiajkl1Ln0rLST8uEy375fsz35d/3Lc9Y3rbCfcXmlcSV0pXXV/mtql6tvbpw9cM149fUr2WvLVn7et2UdefLXMu2rKeuV67vKI8ob9xguWHlhk8VWRXXKgMq92002rh049tNok2XN/tvrttivKV0y4etkq03t4Vsq6+yrirbTtxesP3xjsQdZ3/g/FCz03Bn6c7Pu6S7Oqpjq0/XeNTU7DbavaIWrVXWdu1J3XNpb+DexjrHum37WPtK94P9yv1/HEg7cP1g+MGWQ5xDdT9a/bjxMPNwST1SP7O+pyGroaMxubH9SNiRlibvpsM/Of20q9msufKo3tEVx6jHFh3rO154vPeE7ET3ycyTD1umtNw+NeHU1dMxp9vOhJ8593Pwz6fOcs8eP+dzrvm81/kjv3B+abjgfqG+1a318K9uvx5uc2+rv+hxsfGS56Wm9nHtxy77XT55JfDKz1f5Vy9ci7zWfj3h+s0bqTc6bopuPr2Ve+vFbwW/fbw9/w7hTsldrbtl94zuVf1u9/u+DveOo/cD77c+iHtw+6Hw4bNHikefOhc9pj8ue2L6pOapy9PmruCuS39M/KPzmezZx+7iP7X/3Pjc9vmPf/n/1dozoafzhfxF38tlrwxe7Xrt+rqlN7r33pu8Nx/flrwzeFf9nvP+7IekD08+Tv9E+lT+2e5z05fwL3f68vr6ZAK5oP9XAAOqo00GAC93AUBPBoAJz43UierzYX9B1GfafgT+E1afIfuLOwB18J8+phv+3dwAYP8OAKyhPiMVgGg6APGeAB07dqgOnuX6z52qQoRng60hn9Pz0sG/Keoz6Td+D2+BStUVDG//BT1wgxaGkDKWAAAAimVYSWZNTQAqAAAACAAEARoABQAAAAEAAAA+ARsABQAAAAEAAABGASgAAwAAAAEAAgAAh2kABAAAAAEAAABOAAAAAAAAAJAAAAABAAAAkAAAAAEAA5KGAAcAAAASAAAAeKACAAQAAAABAAAH2KADAAQAAAABAAAAlAAAAABBU0NJSQAAAFNjcmVlbnNob3T4AjTcAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB12lUWHRYTUw6Y29tLmFkb2JlLnhtcAAAAAAAPHg6eG1wbWV0YSB4bWxuczp4PSJhZG9iZTpuczptZXRhLyIgeDp4bXB0az0iWE1QIENvcmUgNi4wLjAiPgogICA8cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPgogICAgICA8cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0iIgogICAgICAgICAgICB4bWxuczpleGlmPSJodHRwOi8vbnMuYWRvYmUuY29tL2V4aWYvMS4wLyI+CiAgICAgICAgIDxleGlmOlBpeGVsWURpbWVuc2lvbj4xNDg8L2V4aWY6UGl4ZWxZRGltZW5zaW9uPgogICAgICAgICA8ZXhpZjpQaXhlbFhEaW1lbnNpb24+MjAwODwvZXhpZjpQaXhlbFhEaW1lbnNpb24+CiAgICAgICAgIDxleGlmOlVzZXJDb21tZW50PlNjcmVlbnNob3Q8L2V4aWY6VXNlckNvbW1lbnQ+CiAgICAgIDwvcmRmOkRlc2NyaXB0aW9uPgogICA8L3JkZjpSREY+CjwveDp4bXBtZXRhPgpZh6eFAAAAHGlET1QAAAACAAAAAAAAAEoAAAAoAAAASgAAAEoAABWRvSWBDwAAFV1JREFUeAHs3XuQVuV9B/DfC4sLyy2s3BYBua7cEQTRaAAxjaOOHSVo45hY02Y0MWrt4MRcpjpNzXXK1KoxkaaNpWlMFTHTRqk2CphxlFEEllvlqiAsNxcXloWVy/Y97wwvvi7CWbK7Wcvn/ec95zm/85zzfvaZ3Z33O89zMn37DagPLwIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQOCkAhkB+0l9HCRAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAjkBAbuBQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEUggI2FMgKSFAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgJ2Y4AAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECKQQELCnQFJCgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQE7MYAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBBIISBgT4GkhAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQICNiNAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgkEJAwJ4CSQkBAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIEBCwGwMECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQCCFQKZv3wH1KeqUECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBM1pAwH5G//h9eAIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBBIKyBgTyuljgABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgTOaIHMOX3PtUT8GT0EfHgCBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQSCMgYE+jpIYAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEzngBAfsZPwQAECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgEAaAQF7GiU1BAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIHDGC2T6nNPPM9jP+GEAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgROJZAp6yNgPxWS4wQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAINO33wAz2I0DAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBwCgEB+ymAHCZAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAomAgN04IECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECKQQE7CmQlBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQG7MUCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBFIICNhTICkhQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQICdmOAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAikEBCwp0BSQoAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEBOzGAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQSCEgYE+BpIQAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECAjYjQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIJBCQMCeAkkJAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAQsBsDBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEAghYCAPQWSEgIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgIGA3BggQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQAoBAXsKJCUECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQEDAbgwQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAIEUAgL2FEhKCBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQICAgN0YIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECKQQE7CmQlBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQG7MUCAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBFIICNhTICkhQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQICdmOAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAikEBCwp0BSQoAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIEBOzGAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQSCEgYE+BpIQAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECAjYjQECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIJBCQMCeAkkJAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAQsBsDBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIEAghYCAPQWSEgIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgIGA3BggQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAQAoBAXsKJCUECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQEDAbgwQIECAAAECBAgQIECgkQI9ysdFt8Fjo1OfwXFWj77RrmuvyJR0iejQJTJti3K91R85HHFgb9TX7o1D1Tvig13vRs22DbFnw/LYtXZpI6+onAABAgQIECBAgAABAgQIECBAoDUICNhbw0/BPRAgQIAAAQIECBAg0KoFioo7RL9JV0TpiElR0n90ZHoPyd5v5jTvuT7qt6+P2s0romr14tiy+Pk4XHfgNPtyGgECBAgQIECAAAECBAgQIECAQEsKCNhbUtu1CBAgQIAAAQIECBD4RAl0Hzwm+lx8VXQefkm0LStvlns/Urk29q15Jba9+lzs3lDRLNfQKQECBAgQIECAAAECBAgQIECAQNMICNibxlEvBAgQIECAAAECBAj8PxLoPvT86D95enQaNSWia+9GfbJM/dFcfX2mTaPOi+rtUbNyUWx+eV7sXresceeqJkCAAAECBAgQIECAAAECBAgQaBEBAXuLMLsIAQIECBAgQIAAAQKfBIHizqVRfvWX41MTropMad+PveVM3b44tH1D1FVuiP2VG7PPVt8U+6sqo3b3tjh8oDZ3XlGHkijp3ic6lpZln9U+MDqWDYrissHRrvfgqC/u/LF911e9G++/8VysffYXUbev6mPrHCBAgAABAgQIECBAgAABAgQIEGh5AQF7y5u7IgECBAgQIECAAAECrVCg/8VXxjmX3Rjthkw64d1laqvi4MblsWfNa7Gj4uWo3rrxhHWnaux6zqDoNWZydBt+UbQfNDbqS0pPeMqh9Ytj64InYvOr8094XCMBAgQIECBAgAABAgQIECBAgEDLCwjYW97cFQkQIECAAAECBAgQaGUCI2fcFWdfMiO7HHyvBnd2dNfbsS+7dPu2xfNj19qlDY7/IQ09ysdFn0lXRufsUvRtegxo2FX1jnjvlbmxau5DDY9pIUCAAAECBAgQIECAAAECBAgQaHEBAXuLk7sgAQIECBAgQIAAAQKtRaBDac8YMePu6DRpetRnMoW3tacyqpf9T2xe9FTs2by28FgT73XrXx79p1wfXc//k4huZQW9Z+rro2bxvFg998E4ULWz4JgdAgQIECBAgAABAgQIECBAgACBlhUQsLest6sRIECAAAECBAgQINBKBJKl2s+7fma0H/PZBnd0cMWLseWlJ6Jy+e8bHDvWcPXVV8VFF02K8qFDo1evnlFcXBxFRUVRnw3Ek1cmG9gfPnw46urqYseOnbF23bp47bXF8eyzzx3rosF72djPRL9pN0b70Zc3OHaw4nfx1lOzTntp+gYdaiBAgAABAgQIECBAgAABAgQIEGi0gIC90WROIECAAAECBAgQIEDgky6QhOvDvnBvFI+cWvhR9u7MLsn+dKye93DUHzlSeCy799nLp8V106+LkSNGRMeOJfkwvUHhxzQkofv+/bWxavXqeGbeM/G7F19qUJlp2zZGTL8zu2T95yO69Cw4XrdqYfzvr38kZC9QsUOAAAECBAgQIECAAAECBAgQaDkBAXvLWbsSAQIECBAgQIAAAQKtQCBZFn70zfc3mLl+ZNtbse2Ff4lNL/+mwV1Ov+7auP6GGTFo4MDczPQGBafRkMx037hpUzz15NyY90zDaw6cfG30+dxfRNs+5xX0nsxkXzHnby0XX6BihwABAgQIECBAgAABAgQIECDQMgIC9pZxdhUCBAgQIECAAAECBFqJwAW3fj86XpSdHf6h1+F3lsY7//nT2Lp00YdaI4YPHx4zZ/51jBo5Itq0aVNwrKl2jh49GitXrY5Zs/4h1qxZU9DtOeOmxLl/+rUoOndcQfv+156OJbO/XdBmhwABAgQIECBAgAABAgQIECBAoPkFBOzNb+wKBAgQIECAAAECBAi0EoGRM+6K7lfeHvXZpdqPvZJwfcPcWbFj1evHmnLvX73t1rghO2u9U6dOBe3NtVNTUxNPZmez/+yx2QWX6DVyYgyeMbMgZM9kZ7/vnv9orJr7UEGtHQIECBAgQIAAAQIECBAgQIAAgeYVELA3r6/eCRAgQIAAAQIECBBoJQL9L74yBtzwrYiuvfJ3lCwL//bTsxrMXP/hD74XU6dOabZZ6/kb+MhGMpt94cJF8c1vfafgSDKTfcDnZxYuF1+9I95+8gex+dX5BbV2CBAgQIAAAQIECBAgQIAAAQIEmk9AwN58tnomQIAAAQIECBAgQKCVCBR3Lo3xdz4Y7YZMOn5He3fGlnmzGjxz/dFHH4kJF4w/XvdH2HpjyZtx++13FFw5eSZ7v+kzI7r0zLcfWr843nz47qjbV5Vvs0GAAAECBAgQIECAAAECBAgQINB8AgL25rPVMwECBAgQIECAAAECrURg9BdmRrfP3VpwN+/N/2mseurBgrYkXJ844YKozy7B/sd8ZbJL2L/+xpIGIfvI6++Os6/8WsGt7Xlhdqz49ayCNjsECBAgQIAAAQIECBAgQIAAAQLNIyBgbx5XvRIgQIAAAQIECBAg0EoEug89P4bfNisypX3zd3RwxYvx+kN3Rv2RI/m2ZFn4adMuy+83ZmPLli2xdOnS3JLyPXv2jH379sWRbN+HDh2KPn36xKhRo6Jdu3aN6TJX+9JLCwqWi8+0bRsT73o42o++PN9XfdW7seaxmbF73bJ8mw0CBAgQIECAAAECBAgQIECAAIHmERCwN4+rXgkQIECAAAECBAgQaCUC4//yu9Hpkj87fjd7KmPdnL+JyuW/z7d99bZb45Zbbm70M9d37doVq1evjoEDB8aYMWOiuro6OnbsGIcPH46qqqrc+5IlS2LDhg25milTpkRRUVH+uqfaSJ7J/vjjc+Jnj83Ol5aN/UwMvfnvIrqV5dtqXvmPePOf78vv2yBAgAABAgQIECBAgAABAgQIEGgeAQF787jqlQABAgQIECBAgACBViDQffCYGHHHwxFde+fvpnrBnFj+b9/L7w8fPjx+8sg/RqdOnfJtaTa2bt0a27ZtiyuuuCL3ftZZZ0Uyez0J15NjmzZtys1eT2azl5WVxcKFC3PtN910UyS1aV81NTXx9Tv+KtasWZM/ZeyXvhNdL7s5vx/V22P1I3fG7g0Vx9tsESBAgAABAgQIECBAgAABAgQINLmAgL3JSXVIgAABAgQIECBAgEBrERjzxW/Gp6Z9OX87R3e9Hat+cmfs2bw23/bzn8+OMaNH5ffTbOzduzfWr18fyYz0JJhPloNPntuetCfLxe/cuTN69+4dtbW10b1799i/f3+89957ufZklvs111yT5jL5mooVK+MrXzn+DPlu/ctj5NcfjjY9BuRr3n/pF1Hxyx/m920QIECAAAECBAgQIECAAAECBAg0vYCAvelN9UiAAIEzRiCTyeTChDPmAzfxB+XXxKC6I0CAwEcEioo7xKT7noy2ZeX5I9UL/jU7e/37+f3p110b3/jGPY1aGj6ZoT5nzpyYPHlyTJw4Mfe3MHnWehKqb9++Pd56663cs9fHjx8fHTp0yIXrbdq0ieT3/rFz77nnnigpKcnfx6k2kqXif/zjv495z/wmXzr2S9/OzmL/8/z+kcq1sfi7N8ThugP5NhsECBAgQIAAAQIECBAgQIAAAQJNKyBgb1rPXG/FxcUxatSoGDduXHTp2jVWrFgRFSsqYkfl9tRXGzbsvLhgwsQ4t1+/2H/gQGze/E4sXLAg9ux5P3UfCgk0p0BRSVH0HNcjSspK4qwuxXFgZ23s3Vgd1e/UxOH9h9JfOhPRfWRpdOzXOTr0aB9xtD5qdx6M6k17o3qd8f5RyGHZJWyHDh6ca57/389nv6Q/tfXZZ5fGpy/+dO6cN5cuzS1N+9F+T2d/wsQJcd/990fN3n1x77335mbrnU4/x87p3LlzTM3OAkxeyVK6CxctOnbolO9JwNG1S5dcXXJecn5zv07nZ/Hhe3rggQfi0ksvjV/9+69i9j8df67uh2v+0O3y8vPivPKhuW7qssHPC88/f9Iuk79ZI7JjbNSokdGttDRWVqyMZcuXRWVl5UnP+/DBJDwaNGhQjDv//BgydGhudueyZctyzx5OZnamebVpk4mBAwbGyOzf0mHDh+UCq4ply2PlqlXxwQcfpOlCDQECBHICAydfG/1u+VFeI1NbFasfuj12rV2ab3viiV/G4OzvrbSvurq6eD77+7Rdu3axY8eO3HPVk2eut2/fPhe0JwF6RUVFLkhPQvFp06bFwYMHo1u3brF79+7c77HFixfHhRdemAvn0143qduwcWPceOMX86f0KB8XI+56NOpLSvNtWx6/Nza9fDyEzx+wQYAAAQIECBAgQKAJBZL/f6ddNi0GDBqQ/V+3NPt907uxcuXKeOP1N5rsKk3xPXtj+0hWp7ps6tRGf4b/+u1vT3iO7zhOyKKxFQg09VhvBR/pjLyFpvgu9mRwQ4YMieHDhsXRbG7z7HPPnqz0jDv2fwAAAP//MjKKLAAAQABJREFU7d0HXBTHFwfwhyIqYkOwICpFUcBeYotd/4kt9pbYe++9xBpbook1amzRJNZYEzXGjg17bwh2RLGLIojynze4yx7cHcd5GPR+8//E253dnd37Hv8r+2be2LjmcosmFIsJNG7UiAYMHEip7Ozitblv714aMWI4hYe/irdNqcicORMNHz6cqlStplSpj+EvXtCSpUto8eIlah0WIPBfCOT5Mg/la+xBNilT6D3948tP6PScs/T6eaTe7UqlczFnytvYkxxc0ilVOo9Prz2jgJUB9PjKE516a17p3bs3tW7TRhJUrliRwsT7QkKleInitGDBL3I3fn/Z/s8/CR1i0vZp06dTpUqV5L4LFy6keT//bNJxxnaaN38+lSxZUu7StUsXOnbsmLHd5bbixYrR/F9+IRsbG7k/H/chijmvhXJd+fPnp9//+EOuRr1+LR0jIiKUze/9yBatWrai7j26k22qVLK9Z0+fUdWqVfS2bZsyJfUb0J+aNWuud/uVy5epT58+FBoaqne7UlmwYEGa/uN0cnTMolSpj48ePaQB/QfQ2bNn1Tp9C94+PvT9999T9uzZ421+HRlJM2fNohXv7OLtgAoIQAACcQRKdJ1E6T5rqNZGnNtN/tO7quvVq1Wl774bLz9D1EojCw8ePKA9e/ZQixYtKHXq1HT69Gk6cOAAubu7k6enJwUHB5OzszPt3LmT7t27R3Xr1qWcOXNSdHS0/O/SpUsUEhJCjx49opTivbd9+/ZGzhZ/E7czYsQo2rFzl7qxdP95lLpg7Pv7iyPr6Pi8Yep2LEAAAhCAAAQgAAEIQMDSAjW/rEl9+/WhLE7O8Zo+I74jj58wnq4FXYu3LTEV73ufnc9lTht53N3oz7V/JuZS5b6lxP0s/r6uLbjHodXAcnITsOTfenJ7btZyPZa4F5uQVbv27ahHj570JiqKSpcundDuVrXdBgF2y73eHTq0p27de8gGX74Io8uXr9DLl+GUzysfZc2aVdZfFjfV+vbtqzdI4ZAuHc1fsIDyFygg9w25e5cuXLhA6RwcqLAIWqQV27n8unQpzRIBBhQIfHABGyKvZvkozxe5Ezz1qwev6ORPpyjsjv4AsGtVV/JulT/Bdt5GvaUzc89R6Enjgb0EG/pEdjAnqJtUAXb+MfXt6G8pUgQ9u3XvThfOn39v5Ty5c9OKlavILrUdXb92jZq3+JqiXhvuqMGBYQ5Ue+bNS5ERkdTi6+Z04/qN974OUxow57VQ2uXrXrZ8OXmJQPs/27aJYMkIZdN7P3JHrbHjxlG5cuV12jIUYHd0zExTpkylYsWLy/1fhIURB4CeP39GPr4F1c+v27dvU/du3WTwSKfhdyulSpWi6dOnUVr7dPIHJf+QDgoKJA8PT3Jzd6MUKVKIDmYvadDAQXT48GF9TchA1LDhw8jOLjW9ffuWbt28SVevXqVsItjulc9L/l3wgfN+nksLFy7S2wYqIQABCGgFKkz8i2yy51OrQtZMoitbl6rrc+bMolIlS6jrhhb4Rhl3ELorvp9XFB3cXF1dxXuVHYWJ98zJkyeL9zoPcnJyknUcbOdtb968Ee974aLTkaPc9kJ0igsMDKT79+/TlStX6Ny5c9S/f3+53dB59dUfPXZc/LjtpW7yqtmWsjeJDahHhwSQ3/A66nYsQAACEIAABCAAAQhAwJICtevUpjFjxspOqjxogL/XhoqOqJ7i97+Hp4c81cMHodShfQe6feeOWad+3/vsfFJz28gt7k0tWmzaPQeHdA7qQLu4AXbubIt7HGa9/DjoAwlY6m/9A10uThNHwBL3YuM0qXcVAXa9LLISAXbDNonaUr9BfRo5cpQ8Zv369TRzxgwRnHgu11OKUb5txIjT7qKXB5etW7bQqFEx+8qKd/+MHTuGatepK9fmz5tHSxYvpihxY44L37AbP2EC8f9puPTr14/89u2Ty/gHAh9KwKuFCK7/Tze4/ibiDT27/pzSOqelNI6pdS7lzas35Df4YLyR7E4FHanYgGI6+/JKxMNXZGNrQ3YZdduJfhtNh7/1Nxisj9fQJ1xhTlA3qQLszJwxYyaKiHhFr14ZzsyR2JejS+cu1KlLZ3nY3DmzjWbtaN2qNfXu20fuu1CM0p83f15iT2f2/ua8FnFPli1bNjnCMW69uescJJ80cRI5OTvJJtasXk2ZMmemGjVqkKEAu5KJgINHy379lX7+eR5FRb1WL4F/OH876ltKaWtrMEOAi4sLrRW9u7ljxL2Qe9StW1e6KYLjSuFRnTNnzhKB8myyI0SNGtXoxYuXymb5WKBAflr+2+/yB/rNG9dlNpdLly6r+2TImFG0MZO4ZyaXTp060ckTJ9TtWIAABCAQV8DZqxh5D10hqkUPQf434jmdmtCUnt4Jkuv8z57dO8nePq26rm+BR6Xv379fZtYoUqQI8Xt3KpEdJG3atPT777/LoDt3CvL19ZUj2dOkSUOZMmUS76VRspNQ4cKFZccjPobbuiyygvB77sWLFylDhgxUr149fac1WMcdeCtXic12lTGnBxUduZqiU6d/d4xoe3ILCr1y0mAb2AABCEAAAhCAAAQgAAFzBDggt2bNGnmPgAdGDBk8mAKDYr9f16pZSw7G4Gx6Fy9ekNn1EnseS9xnt0QbCV03d6rdtGmzvAdz9txZatemrXoI7nGoFFj4BASM/a1/Ak/vo3wKlrgXa+oTR4DdsBQC7IZtErVlzZq15O7hTtu2bhWB9pF6j50vUh+XEKliIiMj6IsvvqTnz56p+/GXk7Vr11IKMapx44aNNH78OHWbspA+fXpasWIFZc+Rgy6JG3ItW7ZUNuERAkkukMohFVWc/jmlSJVCPdf1bTcoYM1VorcxVc5FslCh7oUopV1KdZ/AjUEUtCE2JZRNShsqM7YMOeS0V/fhwPrJ2Wfp+fWY/09k8clMhXsUJlt7W3WfR+cf0vEfTqnr1rpgTlA3KQPsSfE6cABi1aqVlDuPmwzeN23SlO7o6fHMo5rXrl0jAhz2xAFZTm/+WvSc/lDFnNciqa/thx9+EEGXKhQmOniNGzeWdu3aTWPfdd7SF2DPIT5PNv/1l7wsQ589vFF5rtwzvapIM8/BHW3p2bMntW3XTlY1atyIbly7rt0slznIvkoE/LkMFCM294hpU7Rl9OjRVPerr2TQqU7dOnTvboh2s1x2EBldduzYIdPeL1mymObMnhNvH1RAAAIQUATijuyOunGKDo5tpmym2rVr0ZjRo+T7jlqpWXjy5IlM9e7m5ibTv2cWHZY4HfyZM2dk51mejuOE6OjDqd65nr+b840He3t74h+7vJ1Hsb98+VIG5G1FRyUeAc+j1/kY7ojEWTrq169PuXLl0pzZ+CK3O2bsePr77y3qjuVGryLbPEXV9bgj9dUNWIAABCAAAQhAAAIQgMB7CIwX9xpq1q4jM9Tx9199WQRbfP01DRgwQJ5lgPj9vzfO7/+ETv++99m5fUu0kdB18rRRPFUsl549euhk68M9joT0sP1jEjD2t/4xPY9P6VotcS/WVA8E2A1LIcBu2MbkLTyq/Gcx4pzTE9er95Xe9O/cWF6RwnjIkCGy3aVilOABMRJGKX379Rc35b6R8xhw+hhOHamvNGnaRLQxVG5q17ZtgnPZ6msDdRAwRyBriaxUpGch9dDnt8LkqHK14t2CZwMP8vjKXa1+cOYhnfwxNjCeTbRTWNMO73hapIC/f/Seegwv5KmZh7ya5tWp8x97VIyWj+2YorPRSlaUQCc/3aSYgz29GEmXP7+XTOvF70N84//OndsiXbfuHFJ8fqcsWUSK85gpLY4dOypTxXM9F+22Q4cOyuAFjxD8rNRnspPQLZFunOfkUjJ9xBwV+2/JUiVp3rz5smK/334xtUbMKPXYPUjM0z2VqlSNGcHXtauYr/1o7Hzt2vPHvTZtGx6iY1T27C6y45Mp871rjzXntdAez3PHpxGdA+7fvyed9W27c/sW3Xg3EpxTpOcXo7xT2aWi06dOi/TrQfGCQhxgdxJz/w4fNkxN5W4swM6jwXludS4zROYVTuumr/D8OnPmzpWbeJqT/X5+OrvxNt6H08jXNzIS8y8RzOdOYn+KjhGTJk3WaWP48OFy5Oc10QN+4sSJOtu0K4uXLCEeDXrxwnlqJTIYoEAAAhAwJFC8w3hyKN9U3fzi8J90fMFwdX38+LH0xf9qqOvaBQ6C8whzzkLFI9V5hDoH03m6C+6cxI9cx+nfeb/FIvNU06ZN5fd9DqTzf+nE9E48ip2D6Vx4f56Cgz9fuZ6zv3DKeA6Yf/PNN9rTJ7j8z/Z/RUas0ep+JTpPpHRlGqnrYQdW04lF8TNmqTtgAQIQgAAEIAABCEAAAokU4MFfO3fulAPE9P2uV5qztU0lOoNulvOzHz50iDgQZGqxxH12S7SR0PWmTp2aNm3eRFmyOMkOuO3fDTpQjsM9DkUCjx+7QEJ/6x/78/tYr98S92K1z51f5xIlSlJOVxcxMDiMzp8/J+/zcvY9BNi1UrrLCLDrepi19sMP34sRg1Vpn+iNx/MomlM4vSTPve7v7089xFzGhgp/kdm9Z4/cPEekTl6yeImhXVFv5QJpndJIgXAxF7olSs4KLjqB87uH7tLVdbEpoJRzZCmYhYoPiB1B9fTaMzoy7qiymXzb+5BLhRzq+pPAp3R0QmxgVNmQwjYFlZ9UltK8ex5cH7hBjIbfGDsaXtnXmh7NCeqaMoKd5+EeP+E7GSSN6xkgggxDhw+LNyq5ukg7zvPOcuE04toRx9ptZcuUoS+/+IL69R9AGTJmUJvnQAO/bw4dNlzvPOtKr2g+YJDoEbx792712PKffy4DwlyxdcvfIsjwrbqNF7Tnj3tt2h0HDxksAiLN6EHoA/ryyy+0mxJcNue10Daq9KbevGmTGGU+VrtJ7Wm9VASTea72SZMmxXttOAvK0KFD5eeGcvDnFSrQ4UOHdVK8GwuwK8cl9MjzDU//8Ue5GwfktR3EuLJ69eoyHXJw8F06ePCAweY4C0s+Ly/6559/aIQIqJtTfvvtNyrg7U0Xzp+n1q0RYDfHEMdAwFoEygxdQnZe5dSne3/993Rp80J1fdXKP0THHjd1XVngwPcWMaVT165d1eA414WGhsqOYc/E+y+PVOcR7fzIHZ5WiwwdFcR7MI9E54A8z73O25VU8Rxw56A6j1rnwPvjx4/lPtzRbN26dTRlyhTZlnINCT1eE5lCmjX/Wt2tQN2OlLXBIHU98spBOjy5nbqOBQhAAAIQgAAEIAABCLyvQKVKlYinmePSoX17Oi0GThgq/cUI9q/FSPbw8JdUpVJldRpUQ/sr9Za4z26JNpTrMfT4teggq8QBunfrRkeOHDG0q9F63OMwyoONyUDAUn/ryeCpfFKXYMl7sZx1pGPHDnIqWC0SD44bOHAA1RNZ93qI6a/fiHsaPMAKJVYAAfZYC7OXtm7dRs5ZnWm5CILM+Okn2Q7fbHPL4yZ66mWhkHshIl3Odb0jQHlnB3GTbZcImvNIGJ4vl2+wGSs7du4QN+syyyBG7169je2KbVYqwMH1kkNKyGd/bMpxslSQ3RRO18o5ybtNzKhm3v/OvmC6sOSiemjF6RUodWY7df3y71fo5o5b6rp2Ie6c70+uPqGj3x3X7mJ1y+YEdRMKsPMo7tlz5lLWrFml55Mnj+mUGCFdQHT6yS7SsHN59SpczCXVjgKuBsh1/sdYEFu7beSIkTROTHvB73G3bt6gyNdRlEdMi8HzcXHZvWunDBS/efNWriv/ZM6cif78c70MyoeEhFCjhg1FyvgIcZydTA3v6uoq5xVv1KiBCFQ8UQ6Tj9rzf8wB9t/EnOQlihclbx9f+aP0WtA1csnpIj8D+Iny69JTfME5dSo2S4QOhFixRIB9+IgR1FD48xepOnXqGMzUEvfc2nXOYLBnz17Z03327Fm0dMlS7WaTlp3F6PwtYioWHu1prLe8SY1hJwhA4JMXqDBpC9lk81SfZ9Cc7nT7+E51fc/uHTKdu1rxbiEgIEAGwUuUKEEPHz6kQ2LUDf/Hn2Oc+SOLyODy6NEjuff27duJU8kXKlSIeBqLnDlzUr58+eT71IsXL2SwnXuCc4CeO5bx/OwcoOd1JWjP2T1GjRqlfubGvR5965x2vnKV6uom1xLVyKNHTKYRroy+F0h+w2qp27EAAQhAAAIQgAAEIACB9xXo27cftWwVM2Vp9WrV5PdgQ202adKYhgwdJje3adOazp87b2hXnfr3vc/OjVmiDZ2LirPC3+l59LqjYxaRYfAUdejQIc4epq3iHodpTtjrvxOw1N/6f/cMrPvMptyLVQafKVIvwsIo+G6wyM4Xc18jKDCIdop79506dUKAXUHSPCLArsEwZzFlyhTihtthGTCYOPE7+vffHeJDtSM1bdKE7FLHBhH5j3LRwkVifvUN8U7j6eFBq9askfUzZ8ykZct+jbePtmL58mUy2MJ/3E1FyngUCGgFlOC6MvL7lRjB/qGC7CLmRSWHl6RMeTOql3Rh0QW6s/+uXOf526stqKJu44VTs85Q6IlQnTplJff/clP+FvmUVYp8Gkl7+/qp69a4kBQB9mHDhlKjxk3o6dOnolfaQDp18qSaetzbx4dmz54terBlpEMHD1KvXr1UdmNBbO02Hh3O6XO/+24CPXjwQB6fLVs2mQa8SNGicj3uCHXlJNxDjoMOXOaLqTh++eUXai3Sgvd+lzJ+woTxtGF9/PdV7fk/5gD7o0cPZZCHOyn47dun9viu36A+DRXThXAnhYCAK9SieQuFLN7j+wbYO3bsSF3EKE4Oaq9evYqmTpka7xymVHQT2Vn4R+dbMR9xi+bNKVCM+ExMcXXNKbIWzKQ8bm4ULoJWzcVcZ3fu3ElME9gXAhCwMoGKM8R0TOmd1Wd9drzokHXtgrp+6KCfOkJdrRQL/N6yT7zn8vsef25xincOnnNqNA6m81QWHDznEekcdHcT70u5RccxTh2vjG7nm2U8Up2P4c5qfGOCP2e5MxsH2nlKDR75fuvWLdlmDZEVpkiRItrLMLrMc7uXLVdB3Sezuw8VGrVeXafnobSvz+ex61iCAAQgAAEIQAACEIDAewrwoLBqIoMdj0qv8Hnsd1F9zXKGvZ/eDUQbKqZM3bFjh77ddOoscZ/dEm3oXJSeFe19qbhTFurZXW8V7nHoZUFlMhOwxN96MntKVnU5Cd2L1WYl4fvLP/34Ex0/fkJmRuV7GnXq1pEj13mAV5o0aRFg1/PXgwC7HpTEVPFNMh5Nx2XI4EHUpUs38vD0kOscREjxbs5FWSH+mTZtGq344w9lVT7yPLwLFsakqzTlC4fyZebhwwdi3sjEpTTWOTFWPjmBuMF15Ql+qCC7R1038mwYO1Ls2c3nMj189JtoeSlpHNNQhWnllcuSj4dHH6HnYj99JWsJZzHve2F1U7SYB3xHx11iWJZaZXUL2gD7urVrKVLc3E+oZBUfiFVFz2IuPAfUdpGeWykcPNi2baucF2vJksU0Z/YcZZP6WFmkAKtQsRIdPXqUtm/fpmbjMBbE1m7jeb07duwULw08f1CvW79OjO6zF3PXLqK5YhR93MLXx0H1ouJ9kj/Mm4iOAJxm3EFMl8G9hDn4y8GLuEV7/o85wM7PS9+c51zfT0xJwnP28qhyTuHOo/v1lcQG2Nu2a0tOTs7kmtOVfHy9ZY9s/vE846cZIqPAn3q99Z1XW1dAzB2/dOmvskOAqUF6nnP+q/r1KK34AsfHe3p6yuOPHTtG40RK/eDgYO0psAwBCEAgnkDFBWeIbFOr9f59SlPE8yfq+tEjh8x6T7t3755M686fb5wOnjuNcR0v83RO3CmNU8VzPde5uLjI92ge9c7bOU081+3Zs0cG5a+LTFc8Mr58ed3vSOqF6lngz8dSn5VVt6ROn4lKz/BX1ykqgvZ1jv0OFbsBSxCAAAQgAAEIQAACEDBP4Gcx8IHnNzdl0Jd2QNmkiRPl/YSEzmqJ++yWaMPYdaZNm4Y2iwxUnF2WB6jwfSlTCu5xmKKEfZKTgLl/68npOVjztZhyL1aZvpTvuX/TsmW86WHZT8lqyssc7/zss894EeWdAALs7/mn4FvQl379dZlsJeTuXcouRq7w3ASzZ82iC2LEJt9EKyPmH27fvh3lyp1H7jdZzKW7VgTGlKLtKWJKyhwlHU9kRCSVKxd7Y01pD4/WKWAouK5oJHWQPWdlF/Jp462cTt6w9hdzrz+/Hhs8T5/LgcqM052nY28vP4oMi1SP0y5kcMtApUeX0lbR7h57KeplwkFlnYM+oRVtgN2cpxU3wJ4ihQ3t8/OTvdB4viieN8rUYiyIrd02VfRw5rlp9ZU5c+ZQafEeeVik3u3Zs6e+XYhT2P/xxwoZXL0Xco+yZc8mgvWv6RsxP4yhUdDa83/MAfanT59QtaoxnSPi4hQuXJgWiznaubRr25bOnj0bdxe5ntgA+6pVq8gzb16dtr4dOUp0JtuiU2fqCgeYOLjOU6k8fBBKjZs0JZ4/PqFSrVpVmjL1e53deN71niKLwjMxChQFAhCAQEIClUQWnWiblHI3m+i3tL9LEdEpKfY7xxH/gwk1YXA7d+4KDAykDSI71V3xG8DR0VEG2fm7P49i58Idg/g/TinP79E8wp3nZucR8pUrV5aj37kN7iDFQfnatWsbPJ++DZ+VLqdWp7S1o8/nnxbPN4Wss4l+Q3s7+KjbsQABCEAAAhCAAAQgAIH3Ffj9998pv5hO8ODBA5TQtKWcmnifn8goJYqp08RZ4j67Jdow5tSmTRvq1TtmytauXboQDwIwpeAehylK2Cc5CZj7t56cnoO1Xosp92L5fvvqNTExyjlzZtOSxTH3mOOa8ZR327Zto/QZMiDAHhdHrCPArgclMVXa4Dgf9++//9LIEcMp7lzC2URqyEWLFskUka8jI6lmzZrqPDXlypWnmbNmytN26dxZpGEwPse0ks45/OULqlChYmIuF/t+ogIJBdeVp51UQXYOrvO86zbif0rRN7e6g0s6KvtdGWUX+eg34AC9evRKp05ZyeyViUoOi5lLXqnb2WU3vY3Unatb2WYNj9oAO3+J5xSxCZUMGdKTt3fMTfa4AXY+Vgly8/LmTZto5cqVdPnyZV41WowFsbXbWooecJdEhyN9ha+nYaNGcm6Xr+rU1beLrOveo7voqNRB3W5otL2yg/b8H3OA3W+fH/Xr11d5WjqPTiJgs03M/ctl7OjRsge1zg7vVhIbYB8ksrFkz56DXEQgyN3dXXZs4KY4CPT991Pp2FHTfjzyMQ4iPfJCkZ2A5+3hTmGdO3ciHvFpSuEOBK3FD1d7e3txvKccSc/HRUS8kvOpTZ40WaYsMqUt7AMBCFinQFIG2LWiPHqdp0LhOdV5TnZvb2+ZDp5Ty/Pn9P3798UIl0wyrTyPbOfPWJ6/jB95VLubSDHPWTlatWqlbTbBZQTYEyTCDhCAAAQgAAEIQAACFhT49ddfyVdkXjpx4jh17tTZaMuOjplpu5hKlcsMkXZ4+W/Lje7PGy1xn90SbRi6UO40wKPXM2bMZJKBth3c49BqYDm5C7zP33pyf26f+vWZei+2YaOGItPtCMnRSUzpeVJkijVUOLtsseLFEWDXA4QAux6UxFTx6PTZYgQmFx6RV79+A3FDLTb1pLatCiKF748//iirtCM6OSXkUvEFhcvYMWNo8+bNctnQPxyM56B8SEgI1UnkSBdDbaL+4xUwNbiuPENLB9nz1MxDXk11R7sGbbpGgevjz6+cKr0dVZ6pO0fTsYnH6XGA/v/P5CibnQp29lUund5EvqFdXfao69a4oA2wVxbvKWFiDtiESvESxWnBgl/kbvoC7Hnc3WiRmKaC01sp5fad23RapLo64n+U9u7do/c8xoLYxrYp5+DHwUMGU9OmzRJ8P+Pechs3biInZyc5Cvqrr+oZTInO7Sb2/DxP/JdfJm7KDXNeC742pShpeLhTw1iR8lxbjG1T9uMfdDt37ZSrnDJ9k2hHX0lsgF3bhm0qOypXtoy8Pu6p+CIsjFq3baM3ZZD2OF62s7MTn4+zqXjxEnK+Yf7b2yE6oZlbnJycaMDAgcTzFHNZt24dTfzuO3Obw3EQgIAVCCRVinhjdDw3O4/sef78uQye8+h1HtXO72GHRLaWDOK9lEexc6CdP9t4XnZlrvbOoqOtqQUp4k2Vwn4QgAAEIAABCEAAApYSmCUytpYtV444i2udOnWMNqsdSW7snoW2EUvcZ7dEG9pr0i63Exlqe/ToKat4AMEJMVexuQX3OMyVw3EfQsCSf+sf4npxjhiBxNyL5U7/Xbp2lQdWqVLFaLbR4cOGUcPGjRFg1/OHhgC7HpTEVGmD43t276aB4ua/ocK9R3aJuRZTpEhBf65dQ5PE6DsuuXPnFvMQr5fLv8xfQPMXzJfLhv5RAi9XxKiXr0WKZBTrFUhscF2RslSQPW9DD3Kv6640Kx8DNwRR0MZrOnXqishaWn1RVZ2R7mfnnacQ/xB1F+1C3DndIx6+on0DD2h3sbplc4K6CQXYGTFLFkfq1r0HVRdztfP85toSGRlBK1espLk/z9OZR91YENvYNm3bpgbY+Zjly5eRt4+vGCV4gVq1ND7KL7HnR4Bd+6rEX+ZA0MpVK0UwKA0d2H+A+vSJSYcWf8+YmpQpU9DU73+Qvc+5RtupzNAxptZr5/5p3bo1cdp4FAhAAAL6BCrOECkp0zurm86Ob0CPr11Q1w8d9CMeZW7pcl3MqT5t2jTiH6kcCOd5IMNEByUerR4VFUWurq5yVHtQUJAMtvMoG04bz+9pphYeGV+2XGynxczuPlRoVMzvCdnG81Da1+dzU5vDfhCAAAQgAAEIQAACEEhQYMKECfSlyMrK8/CWK1uWosSjoaK9LzOwf3/as3evoV3VekvcZ7dEG+oFaRbSpbMXg+L+pgwZM8i08Jwe3hIF9zgsoYg2LCmQVH/rlrxGtBVfILH3Yvv2608tW35DfN+/XNnY6efit0wyAx8H4zEHe3wdBNjjmySqhnubbfvnH3kMj1b5cfp0o8f//fcWOX8wj+IbOnSo3NdW3NjbLUaIpk1rT3//tZlGjx5jtI39B/bL+ZK3bvmbRo361ui+2AiBJBGwISrQ0otyVc2l03zAqqt0fdsNnbq4K6XHfEYZ8sQGcA2NdufjCnbypRzlsqtN3D8eSqdnn1HXrXEhqQLsiiWPWC5atDAVK1qMSn1WSo4+VrZt27qVRo4cqawaHSWu/SFlSop2UzJyJEWAfciQIdSkaVNCgF19WQ0uzJgxk8p/Xl6OtqxWtarB/TiYNGbMaKr9LuX//HnziFMJWar4+PrSsmXLZHOWDNxb6vrQDgQgkHwEKkzaQjbZPNULCprTnW4f36mu79m9Q05DoVZYcIFH9/j4+Mi52dOkSSMztfC0G7du3ZIZPjhlPKeOPyXSsHl5eREH5bsk4ibdy5cvqXKV6uoVu5aoRh495qrr0fcCyW9YLXUdCxCAAAQgAAEIQAACEHhfAe2o1q/q1pXTHBlqUzt/c/16X9Ht23cM7arWW+I+uyXaUC9Is9ChQ3s5MIWrOnfsSCdE1kdLFNzjsIQi2rCkQFL9rVvyGtGWroA592Lbi4wc3d9l5Khdqxbx1HeGyvhxY6lm7ToIsOsBQoBdD0piq/4Sc69kF3PVnhIfrB3FB6yhwiNE/9kekx6X5zj+4fvv1V2VOZCDAoNEuuQman3cBU8PD1q1Zo2snjBhPG1YvyHuLliHQNIKiFHovu19yaV8bOCbT3h5RQDd3H4zwXPnbeRJ7nXc1P0in7+mA0MPUtTLKLWOF+yd01C5SWXJRoyEVcrFXy/R7T0JfyFX9v8UH5M6wB7XLJ+YO3us+BD1yp9fbmrSpDFdC4rJUGAsiG5sm/YcSTWCXTvnVlNxzUHvrll7bl6ePXs2lRG9rq01wJ4tWzYZ6OERlXdFijdjpXuP7tS+fQe5S43q1ejxY/1TOyg9IHlHUzqe8X4cfHJ2jhll+ujRQzFP8Uuu1ltSpUpFBw4elNlg1orPw8mTY7LB6N0ZlRCAgFULlBm6hOy8Ynti31//PV3avFA1WbXyD3J3d1PXLbnw999/i5uIt+Vo9S+++ELOzZ4xY0aytbWlDRs2iJ7iLeWIdq7buXMnPXnyJN50Icau59q169SseWwmqwJ1O1LWBoPUQyKvHKTDk9up61iAAAQgAAEIQAACEIDA+wrwPOKLlyyRzQwVAxZ27NhhsMkpU6ZQterVKfR+KNWs+aXB/eJusMR9dku0ob0uzkrLc6/z1HlHjxyhbt26aTfHW8Y9jngkqPhIBBL7t/6RPK1P/jLNuRdbV3SSGj1mjLTp1bOnnNLOENTy35aTt7cPAux6gBBg14OS2KqBgwZR8+bN6XVkJH3x5Zf0TMylqK/UrlWbxo4fJzeNHT1afjAr+339zTfUX6TL4cLzH5w8oX8OF2W05du3b6l+vXpGewoqbeMRApYSSGGbggp3K0jOxWPTrXLbl367Qrd23jLpNOnd0lOZ0Z/p7Ht9yw0KWHNVp65I90KUtVRWte5t1FvaP+ggRTyJUOuscSEpAuwpUthQHjc3NXAe15XTxk+eOlVWDxk8SAQCdsllY0F0Y9u07SdVgD2H6PTEP364DBJTd+wWU3jELfy8d+/eQ+kcHKw2wD5VvK5VxevLKX5q165NoaGhcZnUdf4RzT+mIyMi6fPPy4l51aPVbcqCtjf7xg0biTuCRUfH30/ZX3n09PSkVatXy1XtFCrKdu1j6dKlac7cmFGac8Uc74sXx/y41+6DZQhAAAIsULzDeHIo31TFeHH4Tzq+YLi6Pn78WPrifzXUdUsu3LhxQ2bb4Ck2KlasKDN5cKCdg+kcVC9VqhS9fv1apojnUeyR4nfE+PHjTb4E7rQ7atRodf8SnSdSujKN1PWwA6vpxKJR6joWIAABCEAAAhCAAAQg8L4CtrapxOCxf+T32WPHjpGhNOncgX7z5s1kKzrIb9q4icaJgRumFkvcZ7dEG9rr1c5V3KlDBzopvr8bK7jHYUwH25KzQGL/1pPzc7GWazP3Xmy27NmJBwZwMXYvlqe827BxgxiglRoBdj1/VAiw60FJbBWneNy4caMM0pw5c4a6i15sr1690mnGyys/LRBzq/Pcxi9fhBHfYAsPj93H3j6tCAb9Lb+gXL92TczB2IpevgzXaaNY8eI07+efKaUY+bJ1yxZxUw03zXSAsJKkAintUlDRPkXI0cdR5zzPbjyjYD/9c6grOwb73aE3kW+VVSrctSBlK51NXY+maLq96w6FngglG1sbcimbQ2c776gvCK82YEULlg6wlytXjiZNmiTfv7TBcy0pp3gfOzamc1C9r76S88TydmNBdGPbtG0nVYCdz7FzFwcxMtGZ06fFyOv22tPKZW3KI2sdwd5EZEwZMmSo9ODR4FOnTtEbOK9atQpNnjJVjhrfLVwHDRocz7NBwwY0YsRIWc/ToAwfPkxvW/EOFBU8VcrfW7dQlixO8jOylZiH+Mb1+NNNpBefodOmT5NTF3Dg/psWX9OVgCv6mkQdBCAAAfKq2ZayNxmmSkTdOEUHxzZT12vXrkVjRo8yqSOQepCJC9wZdtiwYdSsWTOZhv60+Czi9zAOvHP6tuLie/2+fftExo4XlDp1avnbYcy73uMJnUKmfxs7XvwY3qLuWm70KrLNU1RdD1kzia5sXaquYwECEIAABCAAAQhAAAKWEGjbri317NlLNsUdRDeK7EzawoMZeIq5suJ+E38n5kyIcX/fcwC+kuiEytnp4mbTs8R9dku0oTwnBzEog0fE8z39I2L0Ot/3T6jgHkdCQtieHAXM+VtPjs/Dmq7pfe7FstPixYupcJEi8p5Izx49yN/fX4eP38/n/jyPSpYsKesxB7sOj1xBgD2+iVk1rVq2oj79+spjz547K1O3Hz50iPgDvWy5sjINZKZMmeXIv/79+9Hhw4fjnUcb6Lh6NYBm/DSDzp49KwNfVSpXph7du1NakZImXNyI+0aklbx582a8NlABgaQScC7mREV7FzGr+X3991PE49iR52kc01Dpb0uRXUY7k9p7eS+c/Mf6U1T4G5P2/5R3snSAnW/qb/5rs5gjNotIT/uYxo8bT8ePH6ewsDDKnTs3lS5TmnqK+Vh4lPeVy5fp669j09EaC6Ib26Z9fZIywK6kI+PzHTx4gBbMXyDmvr1Nnnk9qJoYtd2sWXM5tUfRYsXeewT70KFDKSIi9m9c+xyV5Rtifl3t+/aaNWvJ3cOdNm/aFC8tsLFtSnvceYA7EXAZN3YsbRLt6Ctjx46R86E/e/qMOFCuLZxunacs4QwGXI6LHuic1j0gIEAEe8Ipf/4CVKFiBWnF258+fSLmGutEgUFBvKqWatWq0qTJU2QAnjsrTJo0UY7MVHfQs3DkyFGKinqtbvlKdN74VmR34cLn/mXBL3Ts+DH5Qzy76FXpXaAAtRfTsPBoUC7GelfKHfAPBCBg9QLOXsXIe+gK4WAjLWwintOpCU3p6Z3Y97A9u3eKAHjaJLFasGABlS9fXgbWg4ODxdQaj+VNRu6QW0x89jwVWa8uXbokA+ycOn6gyLhiSuFOuJWrVFN3zZjTg4qOXE3RqdO/q4umi5NbUOgVy8wLqZ4ICxCAAAQgAAEIQAACVi/A351Xigx0Ljlc6I2Ybm7Z8mVylPqDB6GUN28+6i7uX5f67DPptO7PP2nixIk6Znwfatu2bTLdekTEK5E+vla8bLCWuM9uiTb4wrt07kKdunSWz6GDGLzBHWdNKbjHYYoS9klOAub+rSen52BN12KJe7G+BX1pyZKl8n7uw4cPaOnSX2nX7l30UEztUbR4MZFBuz59WbMm3bp5g3LlzoMR7Hr+wBBg14NiTpVtKjsxCnAwNWjQwODhUSIN5KDBg8lPjFYxVPr07UutWrUytFmkj4yg3r16E6fhQYHAhxSwZICdr9s+W1oqOag4pc6SxujTCLsdRse/P0mRzyKN7mctGy0dYGc3nr5i5KiRlMoupsMD90Z7LILtPJpYKSFifm5OsXXp4kWlKtmPYOeRgvPnz1fnj1cv/N0CpykLDr5NXbt1f+8Ae9y29a1zBpKFC2Pn/jUWRDe2TWnbEgF2boudxomOFRxIN1YCAwOpf79+agYD7b5btm4lThmUmMJTD/Ccw9rCHQB4BKd9Ogdttc4yj1yfPXsW/Sq+9KFAAAIQSEigwsS/yCZ7PnW3uCO758yZRaVKllC3W3KBA+yVRSdZzmzFQfXMmTOLjl63KIOYu9FOfObyaPb8+fPLIDuPFujatatJpz967Dj16BEzaogPiDtSPzokgPyG1zGpLewEAQhAAAIQgAAEIACBxApwx/eFixbq3DeK28Z+v/2yA6m2Yz3vU6BAfvrt9z/U3buJ78BHjx5V13nBEvfZLdEGz7n+l0h1z4NO/MVguR5ihGdiCu5xJEYL+/6XAu/7t/5fXru1nttS92I56x7f8+dMeUoJf/mC0tqnk6v8Xr7Pb6/IVDpCdqriqTtRYgUQYI+1sMhS2bJlqa8Ikrt7eMieH0qjHBBfJAIrcb8wKNu1j/zh26VrN+L5WpTCAYU9ovfIzyJAExR0TanGIwQ+mIClA+x84bb2tpSrqivlrp4r3mj2l6HhdPOfWyL9vG56+Q/2hJPpiZIiwM5PNZ/oZTx81Ajy9fHVee/iEcv+/kdoihidzMvaYmyUurFt2jaScgQ7nydz5kwyKFyseAnKLuZl58I98v4QP+aWi17W7dq1o27de1h1gJ1N+EtUC5GdgLOl5M/vpQa4eb71ywGX6fTJ07TwlwUUJjKo6Ctbt24j56zO+jYZrNMXYOed84jMCZ3FD2wf7wLkmiu3+gXvXsg9On/+HG0SP273+/kZbBcbIAABCGgFSnQV06B81lCtiji3m/ynxwayq4sMHN99N159r1F3tMDCokWLyNfXV6a95Iwh+fLlo0MiwxUH2J2cnGTHJB7Z/uzZM0onslTVqJHwfPD8m2DEiFG0Y+cu9QpL959HqQtWUddfHFlHx+fFpsZXN2ABAhCAAAQgAAEIQAACFhLg77MdRZa5evXqqQM2uGnOardo0S+0fv1Gnax12tNOnz6dKlaqRCdOHCcOsL95EzutpHY/S9xnf582uop08PwcubRr21ZmmtVenynLuMdhihL2+a8FLPG3/l8/B2s7vyXvxZb//HM5/UV+kT1UKTzQ969Nm2nq9z9QnbpicN7IUQiwKziaRwTYNRiWXEyTJg15iAA5p8q5GxISL9WNKefi9PLcIzA8PFyOdkko/bApbWIfCCRXgVTp7cg+axqRakSkhxbB9dcvYlNHJ9dr/hSvi0fUubm5i17IjqIzTxDdu3fvk3maPMKa5/968ODBJ/OckuKJcLDd1dVVBHvs6erVIIM/iJPi3HHb5Gtwd/egEPE5itctrg7WIQABUwTcK9anXG2nqLvavHxEF2Z210mfvmLFb+QpOsdauixdulRmtzp58iRdFBlgeOoVfo+NjIyUI9g5g4gy5yT/duA0kgkVnqKjRYuW6m6cBt+n91yKtndU624tHULX9unOhaluxAIEIAABCEAAAhCAAAQsKGBrm4pyuuSgzI6OMuNdaGioSa1zVr64AzkMHWiJ++yWaMPQ9Zlaj3scpkphPwhA4L8QcHXNSTlzusr35v/6nvB/8fzNOScC7Oao4RgIQAACEIAABCAAAQhAINkL2KZOS6W/XU0pc3ip1/p09690evlEdb1hg/o0ePBAnQwu6sb3WOAU8RUqVKCdO3dStmzZ6NGjRzKwzmnhU6ZMKTux8VQZKVKkkB3AWrdubfRs3Els6tQfaN362OB5kVbDKWOVNupxb+5eIf9xTSkqIlytwwIEIAABCEAAAhCAAAQgAAEIQAACEICAZQUQYLesJ1qDAAQgAAEIQAACEIAABJKRQOGWQylT1XbqFb0NvU7n5/SixzevqHULFy6gwoUKquvvu/DmzRtat24dtWnThjZt2iQD6jz/Oo9ed3FxkfOxnzp1Sk4JxWnfeZuSftLQuc+cPSf26axuzpzbi3x7zKIUzm5q3ZNdS+jMb5PVdSxAAAIQgAAEIAABCEAAAhCAAAQgAAEIWF4AAXbLm6JFCEAAAhCAAAQgAAEIQCCZCDh5FiafnrOIMmZXr+jp7mViFPt36rq3tzfNmT2DHBwc1Lr3Wbh9+7ZM/16sWDHiQDoH3K9evUpubm506dIlSps2LfE+xYsXF+nXnorpOKKoWbNmBk8ZFhZGPXr2kanmlZ2KtBohRq9rRr0/DaELs3vRg8Azyi54hAAEIAABCEAAAhCAAAQgAAEIQAACEEgCAQTYkwAVTUIAAhCAAAQgAAEIQAACyUegeIdx5FBeE8B+fJcClo2iu6f91Ivs2qUztW3b2iKp4vfu3UuFChWSI9YfPHggA+o7duyQAfXAwEDKmzcvXblyhbJnzy4D7A8fPjQ4gp1Twy9duozmzV+gXmuOIhUoX+vxRJlzqHVhB1bRiUXfqutYgAAEIAABCEAAAhCAAAQgAAEIQAACEEgaAQTYk8YVrUIAAhCAAAQgAAEIQAACyUTAKV9R8u4yjWwcXdUrenV2Jx2d2YuixehypUye9B1VrVpFWTX7ceXKlVSjRg0KDw+ngIAAcnd3pxs3blBwcDDlzp2bbt68Sfb29rL9HDly0OXLl6lVq1Z6z7dr124aOmyEus1GzN9eqvcsSlOomloX/eg2XZw/gB4EnFLrsAABCEAAAhCAAAQgAAEIQAACEIAABCCQNAIIsCeNK1qFAAQgAAEIQAACEIAABJKRQKHmAyjz/2LnMOdLe7j1Zzq/5iedq5w7dzaVKlmCeG50cwqng9+4cSM1aNCALly4QOnTp5cp4u/evStHsp87d46yZs1Kd+7coSxZssj52XmUe9u2bXVOZ2NjQ0ePHafu3Xvq1Ps26UtZanbTqXu8fQGdXTlNpw4rEIAABCAAAQhAAAIQgAAEIAABCEAAAkkjgAB70riiVQhAAAIQgAAEIAABCEAgGQmkTu9IxXv9RKnylo69qmf36da6aXRt34bYOrHEQfaSJYrr1Jm6cv36dZn23dfXl65duyaD6inFqHM/Pz+ZNp5Tw3PJlCmT3M6j3HlO9rgB9mPHT8QLrrtXrE+5Gg4gypBVvZzXV/3pxKy+FPH8kVqHBQhAAAIQgAAEIAABCEAAAhCAAAQgAIGkE0CAPels0TIEIAABCEAAAhCAAAQgkIwEcpetSW5NhxFlzKZe1Zvgy3T9z2l05+RetY4XOF185cqVEj0n+7///kv/+9//KGPGjHTgwAEqUKAA3bp1i/bt20d169aVI+OXLVtGrVu3ltuPHTtGjo6O1KdPH3l+nnN9z569OmnheUPOYpXIrdEASumSX+4n/3l6j66vnkQ3D22NrcMSBCAAAQhAAAIQgAAEIAABCEAAAhCAQJIKIMCepLxoHAIQgAAEIAABCEAAAhBITgK+jXuTU83uFC1SsCsl6sZJClw7je6dP6pUyceuXTpT06aNycHBQafe0Mrr169py5Ytcj71dOnS0bZt26hixYrk7+8v08UXKlRIBtv5+KdPn5KLiwutX7+evL29qU2bNhQWFkarV6+lefMX6Jwim28p8mw8gGzzFFPrbUQK+wdb59L5tTPVOixAAAIQgAAEIAABCEAAAhCAAAQgAAEIJL0AAuxJb4wzQAACEIAABCAAAQhAAALJSKBE54mUrkwjnSviIPuNTT/HG8nOwe8BA/pRQV+fBEezc/p3Ho3OgXQuZ86cIU9PT+J51x8/fkwZMmSgzZs3U6VKlejUqVOUJ08eunfvnny0T+dA06b9SBcvXtS5Lh65nuerbjrBdd7hxeE/6fiC4Tr7YgUCEIAABCAAAQhAAAIQgAAEIAABCEAg6QUQYE96Y5wBAhCAAAQgAAEIQAACEEhGAmkds1Kh1qMpTeHqOlfF6eKDty+ONyc779SwQX1qIkaze7i7k41m9Lu2gbVr11KXLl1kGnhbW1t69uyZnF/90aNHxOscaL9//z7x6PagoCCKjIykqDdv6OrVINq+/V9tU3KZ51x3+V973bTwYsurMzvo7LKxFP7ofrxjUAEBCEAAAhCAAAQgAAEIQAACEIAABCCQtAIIsCetL1qHAAQgAAEIQAACEIAABJKhQMacHlSg+RBK7VtZ9+qe3aeHB/6kC+tmUbQIfsct1atVpQYNG5Cvj48IlNvLYDrvExoaKoPmtWrVops3b8q08nZ2dvTq1SuZCj4gIEAG1Hn9wYOHMm38yZOn6NjxE3FPQTYpU5JPw16UpbwYZZ8hq872iPN76NLKKfT0TpBOPVYgAAEIQAACEIAABCAAAQhAAAIQgAAEPowAAuwfxhlngQAEIAABCEAAAhCAAASSmQAH2fM3GRBvJDtf5quzO+nWrhV097SfwauuXbsWlSlTmrzy5aO7d4PlXOoFChSQgfQnT54QB9ijxVzpz58/p+DguxR07Rpt27qN9u4z3GaOIhUoV9UWlKZQtXjn5ZHrl9dMQ3A9ngwqIAABCEAAAhCAAAQgAAEIQAACEIDAhxNAgP3DWeNMEIAABCAAAQhAAAIQgEAyE+B08T6N+5JD6YYUHTf1++O79PTUv3Rz7xp6fPNKkl555txelLtSE8pYtAZR5hw657IRQfow/3V0Ye1PSAuvI4MVCEAAAhCAAAQgAAEIQAACEIAABCDw4QUQYP/w5jgjBCAAAQhAAAIQgAAEIJDMBHwb9xYp2RsTZcwW78rehl6n5+f2UrD/Vgq9cjLe9vepcPYqRi6la1L6gpUohbNb/Kae3hMp69fS+bUz429DDQQgAAEIQAACEIAABCAAAQhAAAIQgMAHF0CA/YOT44QQgAAEIAABCEAAAhCAQHIUyF22JuWs0oJS5S2t9/JsXj6iV0Gn6fHFw3TvzD6zU7VzavpshStSZu8ylMajCEXbO+o93+ur/nRn9wq6eWir3u2ohAAEIAABCEAAAhCAAAQgAAEIQAACEPjwAgiwf3hznBECEIAABCAAAQhAAAIQSKYCqdM7klftdpSpZC2ycXQ1eJU2Ec/pdUggRdwNpBd3gygs+Bq9eHSXXj4Ipqjwl/I427T2ZO/kQukcc5CDizuly+FBqXN4UqrsnhSdOr3BtqMf3aYnx7bQlb+XUMTzRwb3wwYIQAACEIAABCAAAQhAAAIQgAAEIACBDy+AAPuHN8cZIQABCEAAAhCAAAQgAIFkLuCUryjlrtiQHETqdsqYPVFXaxP9Vu4fbZMiUcfR0xAKE6nob+5bRw8CTiXuWOwNAQhAAAIQgAAEIAABCEAAAhCAAAQg8EEEEGD/IMw4CQQgAAEIQAACEIAABCDwMQo4eRYml7K1KL13eUqZwytJnsKbu1fo+cUDFHxoCz0IPJMk50CjEIAABCAAAQhAAAIQgAAEIAABCEAAApYRQIDdMo5oBQIQgAAEIAABCEAAAhD4hAVsU6elXKW/IEef0mSfuxDZZM8rnq2Nmc84mqJDrtLLm2fp0QV/uuX/D0VFhJvZFg6DAAQgAAEIQAACEIAABCAAAQhAAAIQ+JACCLB/SG2cCwIQgAAEIAABCEAAAhD4JAScvYpRZs8iYm51T7JzdqVUGbORjX0GorQZyCalrXyO0W+iiMKfUfTLZ/T66T2KDL0t5moPpMeBpyn0yslPwgFPAgIQgAAEIAABCEAAAhCAAAQgAAEIWJsAAuzW9orj+UIAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAgFkCCLCbxYaDIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEDA2gQQYLe2VxzPFwIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEzBJAgN0sNhwEAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAALWJoAAu7W94ni+EIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCBglgAC7Gax4SAIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACELA2AQTYre0Vx/OFAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAGzBBBgN4sNB0EAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAgLUJIMBuba84ni8EIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCJglgAC7WWw4CAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAErE0AAXZre8XxfCEAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAwCwBBNjNYsNBEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCBgbQIIsFvbK47nCwEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACZgkgwG4WGw6CAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAFrE0CA3dpecTxfCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhAwSwABdrPYcBAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCFibAALs1vaK4/lCAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgIBZAgiwm8WGgyAAAQhAAAIQgAAEIFFPNHMAAAK/SURBVAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAwNoEEGC3tlcczxcCEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABMwSQIDdLDYcBAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAAC1iaAALu1veJ4vhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgYJYAAuxmseEgCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCwNgEE2K3tFcfzhQAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABswQQYDeLDQdBAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgIC1CSDAbm2vOJ4vBCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQiYJYAAu1lsOAgCEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABKxNAAF2a3vF8XwhAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQMAsAQTYzWLDQRCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgYG0CCLBb2yuO5wsBCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABCEAAAmYJIMBuFhsOggAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQgAAEIAABaxNAgN3aXnE8XwhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhAAAIQMEsAAXaz2HAQBCAAAQhAAAIQgAAEIAABCEAAAhCAAAQgAAEIQAACEIAABCAAAQhYm8D/AZ+WyiduS6k0AAAAAElFTkSuQmCC)\n",
        "\n",
        "![pic_ranking.png](./pics/pic_ranking.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6JpKI-r_sZU"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DINxdSS5_sZU"
      },
      "source": [
        "# **Instructions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqHYpBAs_sZU"
      },
      "source": [
        "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
        "\n",
        "**Environment recommendations to solve lab 2:**\n",
        "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
        "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
        "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
        "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models.\n",
        "\n",
        "## **Phase 1 (30 pts):**\n",
        "\n",
        "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
        "\n",
        "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**.\n",
        "\n",
        "## **Phase 2 (30 pts):**\n",
        "\n",
        "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
        "\n",
        "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**.\n",
        "\n",
        "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
        "\n",
        "## **Phase 3 (40 pts):**\n",
        "\n",
        "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking:\n",
        "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
        "\n",
        "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
        "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
        "\n",
        "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements:\n",
        "* Your preprocessing steps.\n",
        "* The feature engineering steps.\n",
        "* Explanation of your model.\n",
        "\n",
        "* **`Bonus (5 pts):`**\n",
        "    * You will have to describe more detail in the previous steps.\n",
        "    * Mention different things you tried.\n",
        "    * Mention insights you gained.\n",
        "\n",
        "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
        "\n",
        "**`Things to note for Phase 3:`**\n",
        "\n",
        "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
        "\n",
        "* **Push the code used for the competition to your repository**.\n",
        "\n",
        "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
        "\n",
        "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
        "\n",
        "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
        "\n",
        "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
        "\n",
        "## **Deadlines:**\n",
        "\n",
        "![lab2_deadlines](./pics/lab2_deadlines.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnooVY5k_sZV"
      },
      "source": [
        "---\n",
        "\n",
        "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
        "\n",
        "You can delete the syntax suggestions after you use them.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLh3mzfP_sZV"
      },
      "source": [
        "***\n",
        "\n",
        "# **Project Report**\n",
        "\n",
        "**Syntax:** `#` creates the largest heading (H1).\n",
        "\n",
        "---\n",
        "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
        "\n",
        "## 1. Model Development (10 pts Required)\n",
        "\n",
        "**Syntax:** `##` creates a secondary heading (H2).\n",
        "\n",
        "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
        "\n",
        "### 1.1 Preprocessing Steps\n",
        "[Content for Preprocessing]\n",
        "\n",
        "### **Load Dataset**\n",
        "I loaded the three provided dataset files:\n",
        "\n",
        "- `data_identification.csv`\n",
        "- `emotion.csv`\n",
        "- `final_posts.json`\n",
        "\n",
        "These were merged using the shared `post_id` field to form the full dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### **Data Cleaning**\n",
        "To standardize the input text, I applied several cleaning steps:\n",
        "\n",
        "- **Removed URLs** (e.g., `http...`, `www...`):  \n",
        "  Links rarely contribute emotional meaning.\n",
        "\n",
        "- **Stripped leading/trailing spaces**:  \n",
        "  Ensures uniform formatting.\n",
        "\n",
        "- **Normalized whitespace**:  \n",
        "  Converted multiple spaces into a single space.\n",
        "\n",
        "- **Limited repeated punctuations** (e.g., `\"!!!\" ‚Üí \"!\"`).\n",
        "\n",
        "- **Converted emojis to text** using `emoji.demojize()`:  \n",
        "  Emojis carry emotional signals, so converting them preserves sentiment information.\n",
        "\n",
        "- **Removed line breaks and tabs**:  \n",
        "  Prevents broken sentences.\n",
        "\n",
        "---\n",
        "\n",
        "### **Hashtag Processing**\n",
        "Each post has a `hashtags` field (sometimes empty).  \n",
        "Since hashtags often describe the main idea or emotion,  \n",
        "**I appended all hashtags directly to the text** so that the model could use those emotional cues.\n",
        "\n",
        "---\n",
        "\n",
        "### **Data Sanity Checks**\n",
        "- Checked for duplicate values  \n",
        "- Checked for null values  \n",
        "\n",
        "After training a few models, I realized that my public score had reached a plateau.\n",
        "\n",
        "The score improved only a little.\n",
        "\n",
        "So I tried to apply some lightweight data augmentation to increase diversity, especially for minority classes like fear and disgust.(they have the least data size)\n",
        "\n",
        "However, I could not finish the full training in time before the competition deadline.\n",
        "\n",
        "My Colab broken down multiple times on the last day, so I did not manage to submit the augmented version officially.üò¢\n",
        "\n",
        " (A late submission run showed ~0.01 improvement, but it combined with other adjustments so I cannot confirm the exact effect.)\n",
        "\n",
        "\n",
        "### **Data Augmentation Attempt**\n",
        "Because the dataset is **highly imbalanced**, I tried lightweight EDA-style augmentation, especially for minority classes like *fear* and *disgust*.\n",
        "\n",
        "However, **I could not finish the full training in time** due to repeated Colab disconnects on the final day, so the augmented version was not officially submitted.\n",
        "\n",
        "A late experiment run (not submitted) showed about **+0.01** improvement,  \n",
        "but combined with other adjustments, so the effect was uncertain.\n",
        "\n",
        "The augmentation methods used:\n",
        "\n",
        "#### **1. Random deletion**\n",
        "- Remove some stopwords with small probability  \n",
        "- Never remove emotion-related words  \n",
        "- Helps keep emotional meaning while slightly altering sentence form\n",
        "\n",
        "#### **2. Random swap**\n",
        "- Swap two nearby words  \n",
        "- Makes the sentence structure vary without harming emotion\n",
        "\n",
        "Each augmented sample applied **either deletion or swap**, chosen randomly.\n",
        "\n",
        "I augmented only minority classes until each reached roughly **4000 examples** to balance label distribution.\n",
        "\n",
        "Final dataset sizes:\n",
        "\n",
        "- **Original train**: 47,890  \n",
        "- **Augmented train**: 52,772\n",
        "\n",
        "Label distribution after augmentation:\n",
        "\n",
        "| Emotion | Count |\n",
        "|--------|--------|\n",
        "| joy | 23797 |\n",
        "| anger | 10694 |\n",
        "| surprise | 6281 |\n",
        "| disgust | 4000 |\n",
        "| sadness | 4000 |\n",
        "| fear | 4000 |\n",
        "\n",
        "After preprocessing, I split the dataset into:\n",
        "- **Training set** (with labels)\n",
        "- **Test set** (for Kaggle submission)\n",
        "\n",
        "### 1.2 Feature Engineering Steps\n",
        "\n",
        "[Content for Feature Engineering]\n",
        "\n",
        "For models like LinearSVC, Logistic Regression, and Gradient Boosting,  \n",
        "I engineered **TF-IDF features**.\n",
        "\n",
        "Transformers like RoBERTa do **not** need manual feature engineering,  \n",
        "so this step applied only to traditional models.\n",
        "\n",
        "---\n",
        "\n",
        "**1. Word-level TF-IDF**\n",
        "\n",
        "Key settings:\n",
        "\n",
        "- `ngram_range=(1,2)`  \n",
        "  Capture phrases like *\"very sad\"*, *\"not happy\"*.\n",
        "\n",
        "- `max_features=20,000`  \n",
        "  Prevent overfitting, control vocabulary size.\n",
        "\n",
        "- `min_df=3`  \n",
        "  Remove extremely rare words.\n",
        "\n",
        "- `max_df=0.95`  \n",
        "  Remove overly frequent words.\n",
        "\n",
        "This helped capture emotionally meaningful word patterns.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Character-level TF-IDF**\n",
        "\n",
        "I also built a character-based TF-IDF feature using character n-grams (3‚Äì5 length):\n",
        "\n",
        "It is useful for catching misspellings, repeated letters, and short informal slang like ‚Äúomggg‚Äù, ‚Äúnoooo‚Äù, ‚Äúwtf‚Äù, ‚Äúlol‚Äù, which often appear trending posts.  \n",
        "Also, it helps when the text is very short (which is common in this dataset)\n",
        "X_train_all = hstack([word_tfidf, char_tfidf])\n",
        "These engineered features were used for the traditional models in my experiments.\n",
        "\n",
        "These engineered features were then used for:\n",
        "\n",
        "- LinearSVC (public:0.5805, private:0.5948)\n",
        "- Logreg (public: 0.5952, private:0.6003)\n",
        "- GBoost (public: 0.5136, private: 0.5107)\n",
        "\n",
        "\n",
        "### 1.3 Explanation of Your Model\n",
        "\n",
        "[Content for Model Explanation]\n",
        "In this training step, I first tried linearSVC as a baseline.  \n",
        "And then i decided to try RoBERTa model.\n",
        "\n",
        "RoBERTa is a transformer-based language model.  \n",
        "It is known to perform well on text classification tasks, especially sentiment and emotion detection.\n",
        "\n",
        "Unlike traditional models that rely on TF-IDF features, RoBERTa learns contextual embeddings directly from the text, which makes it more capable of capturing subtle emotional cues.\n",
        "\n",
        "RoBERTa processes text using self-attention, which enables the model to understand:\n",
        "\n",
        "- contextual meaning instead of individual words  \n",
        "- negation (e.g., ‚Äònot happy‚Äô)  \n",
        "- long-distance relationships between tokens  \n",
        "- subtle indicators of emotions beyond keywords  \n",
        "\n",
        "This is important in this dataset because many posts are short, casual, or expressive, and the emotional meaning often depends on context rather than explicit words.\n",
        "---\n",
        "\n",
        "### How I Used RoBERTa in This Project\n",
        "\n",
        "#### 1. Tokenization\n",
        "I used the AutoTokenizer from HuggingFace, and converts each post into:\n",
        "\n",
        "- input_ids  \n",
        "- attention_mask  \n",
        "\n",
        "I set the maximum sequence length=128, 256 (max_length=256 in the end), because many samples are longer than a simple tweet, and I found that increasing max_length improved performance slightly.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Custom PyTorch Dataset: EmotionDataset\n",
        "Since I wanted more control over preprocessing and to avoid issues with the datasets library in Colab, I built a custom EmotionDataset class.\n",
        "\n",
        "- text ‚Üí tokenized inputs  \n",
        "- optional label mapping  \n",
        "- padding and truncation  \n",
        "\n",
        "This made the training pipeline more stable.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Handling Class Weight method to reduce imbalanced labels\n",
        "The dataset is highly imbalanced (for example, disgust and fear have very few samples), which causes the model to overpredict majority classes like joy.\n",
        "\n",
        "To reduce this effect, I computed class weights and applied them inside the loss function.  \n",
        "I used a custom WeightedTrainer to override the default loss.  \n",
        "And this forces the model to pay more attention to minority classes.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. Training Setup\n",
        "I trained the model on Colab GPU with the following hyperparameters:\n",
        "\n",
        "- learning rate ‚Üí 3e-5  \n",
        "- batch size ‚Üí 16  \n",
        "- epoch ‚Üí 4  \n",
        "- max_length ‚Üí 256 (and i also tried 128)  \n",
        "- weight decay ‚Üí 0.01  \n",
        "- using class weights  \n",
        "\n",
        "---\n",
        "\n",
        "#### 5. Evaluation and Final Submission\n",
        "After training, I evaluated the model on the validation set using macro-F1, which better reflects minority-class performance.\n",
        "\n",
        "Finally my best competition results are:\n",
        "\n",
        "- Public score: 0.6641  \n",
        "- Private score: 0.6673 (rank around 60)\n",
        "\n",
        "This outperformed all my TF-IDF-based models\n",
        "---\n",
        "\n",
        "## 2. Bonus Section (5 pts Optional)\n",
        "\n",
        "**Add more detail in previous sections**\n",
        "\n",
        "### 2.1 Mention Different Things You Tried\n",
        "\n",
        "[Content for Experiments]\n",
        "Besides the final RoBERTa model, I also experimented with several traditional machine learning models using the TF-IDF features I engineered earlier.\n",
        "\n",
        "Each attempt gave me a better understanding of how different models behave with short emotional text.\n",
        "\n",
        "---\n",
        "\n",
        "### Traditional ML models (using TF-IDF features):\n",
        "\n",
        "---\n",
        "\n",
        "### 1. LinearSVC\n",
        "this was the first baseline I tried.  \n",
        "LinearSVC is a linear version of Support Vector Machine (SVM).  \n",
        "It is optimized for high-dimensional sparse features like TF-IDF, which makes it a common baseline for text classification tasks.\n",
        "\n",
        "#### My Observations:\n",
        "- the training speed was very fast  \n",
        "- the model captured keyword-based emotions quite well  \n",
        "- it struggled with minority classes (e.g., disgust, fear)\n",
        "\n",
        "#### Results:\n",
        "- Public: 0.5948  \n",
        "- Private: 0.5805  \n",
        "\n",
        "---\n",
        "\n",
        "### 2. Logistic Regression\n",
        "Logistic Regression with softmax performed surprisingly well because it balances interpretability and flexibility.\n",
        "\n",
        "#### Findings:\n",
        "- bigram TF-IDF helped a lot (phrases often carry emotional meaning)  \n",
        "- more stable than SVM on minority classes  \n",
        "- but still limited by keyword-only understanding  \n",
        "\n",
        "#### Results:\n",
        "- Public: 0.5952  \n",
        "- Private: 0.6003  \n",
        "\n",
        "It slightly outperformed LinearSVC.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Gradient Boosting (GBoost)\n",
        "I tried using the TF-IDF vectors as input, but the performance was much lower.  \n",
        "I think tree-based models don‚Äôt work well with very high-dimensional sparse features.\n",
        "\n",
        "#### Observation:\n",
        "- training was slow  \n",
        "- the model didn‚Äôt learn meaningful patterns from TF-IDF  \n",
        "- performance was the lowest among all experiments  \n",
        "\n",
        "#### Results:\n",
        "- Public: 0.5136  \n",
        "- Private: 0.5107  \n",
        "\n",
        "---\n",
        "\n",
        "### Data Augmentation Attempt (late submission)\n",
        "Since the dataset was highly imbalanced, I tried a lightweight EDA-style augmentation:\n",
        "\n",
        "- random deletion of stopwords  \n",
        "- random swapping of adjacent words  \n",
        "- applied only to minority classes (fear, sadness, disgust) until each reached ~4000 samples  \n",
        "\n",
        "#### My best-performing hyperparameter setting:\n",
        "learning rate = 3e-5, max_length = 256, 4 epochs , and class weights to handle imbalance.\n",
        "\n",
        "This version unfortunately did not finish training before the competition deadline since my Colab disconnected several times and I couldn‚Äôt submit it in time.\n",
        "\n",
        "However, a late submission score indicates that this attempt improved by around ‚Üë0.01.  \n",
        "However, since other hyperparameters were changed at the same time, I cannot confirm how much of the improvement came from augmentation alone.\n",
        "\n",
        "Even though this experiment wasn‚Äôt officially submitted, it still helped me explore more ways to handle data imbalance.\n",
        "\n",
        "\n",
        "### 2.2 Mention Insights You Gained\n",
        "\n",
        "[Content for Insights]\n",
        "From all the experiments, I learned a few things:\n",
        "\n",
        "### 1. TF-IDF + simple models can reach the 0.59‚Äì0.60 range, but it‚Äôs hard to improve much beyond that.\n",
        "This helped me understand the limitation of traditional feature engineering.\n",
        "\n",
        "### 2. Model performance is very sensitive to data imbalance.\n",
        "Using class weights helped RoBERTa learn better minority classes like disgust and fear.\n",
        "\n",
        "### 3. RoBERTa requires careful tuning, especially sequence length and learning rate.\n",
        "Small adjustments made noticeable differences.\n",
        "\n",
        "### 4. Data augmentation for text is tricky.\n",
        "Sometimes it helps, but sometimes it just adds noise.  \n",
        "In my case the effect was not very clear, but the attempt gave me more understanding of the dataset.\n",
        "\n",
        "Overall, I realized that transformer models benefit less from feature engineering,  \n",
        "and most of the improvement comes from preprocessing and training setup.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZL1hRkl_sZV"
      },
      "source": [
        "**`From here on starts the code section for the competition.`**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEktn7OS_sZV"
      },
      "source": [
        "# **Competition Code**\n",
        "\n",
        "## 1. Preprocessing Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57S1BuR5AKrR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "nyZXZD6h_sZW",
        "outputId": "37a8b802-5205-4a15-a8e8-7411e805ad62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        post_id                                               text  \\\n",
              "1      0x35663e  I bet there is an army of married couples who ...   \n",
              "2      0xc78afe                         This could only end badly.   \n",
              "3      0x90089c  My sister squeezed a lime in her milk when she...   \n",
              "7      0x2ffb63                                Thank you so much‚ù§Ô∏è   \n",
              "9      0x989146  Stinks because ive been in this program for a ...   \n",
              "...         ...                                                ...   \n",
              "64164  0xd740f2                  why is everybody seem sp serious?   \n",
              "64165  0x99267e  You can cross fuck off, its 10f all winter in ...   \n",
              "64166  0x4afbe1  Guilty Gear actually did that before with Guil...   \n",
              "64167  0xf5ba78                       One of my favorite episodes.   \n",
              "64169  0xb5a35a  Texans and Astros both shut out tonight. Houst...   \n",
              "\n",
              "                                hashtags  split  emotion  \n",
              "1                                     []  train      joy  \n",
              "2                                     []  train     fear  \n",
              "3                                     []  train      joy  \n",
              "7                                     []  train      joy  \n",
              "9                                     []  train      joy  \n",
              "...                                  ...    ...      ...  \n",
              "64164                                 []  train      joy  \n",
              "64165                                 []  train    anger  \n",
              "64166                                 []  train    anger  \n",
              "64167                                 []  train      joy  \n",
              "64169  [texans, astros, sadness, losers]  train  sadness  \n",
              "\n",
              "[47890 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-78acd3f7-59c1-4aef-886e-a693e7b7b140\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>text</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>split</th>\n",
              "      <th>emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0x35663e</td>\n",
              "      <td>I bet there is an army of married couples who ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>train</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0xc78afe</td>\n",
              "      <td>This could only end badly.</td>\n",
              "      <td>[]</td>\n",
              "      <td>train</td>\n",
              "      <td>fear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0x90089c</td>\n",
              "      <td>My sister squeezed a lime in her milk when she...</td>\n",
              "      <td>[]</td>\n",
              "      <td>train</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0x2ffb63</td>\n",
              "      <td>Thank you so much‚ù§Ô∏è</td>\n",
              "      <td>[]</td>\n",
              "      <td>train</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0x989146</td>\n",
              "      <td>Stinks because ive been in this program for a ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>train</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64164</th>\n",
              "      <td>0xd740f2</td>\n",
              "      <td>why is everybody seem sp serious?</td>\n",
              "      <td>[]</td>\n",
              "      <td>train</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64165</th>\n",
              "      <td>0x99267e</td>\n",
              "      <td>You can cross fuck off, its 10f all winter in ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>train</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64166</th>\n",
              "      <td>0x4afbe1</td>\n",
              "      <td>Guilty Gear actually did that before with Guil...</td>\n",
              "      <td>[]</td>\n",
              "      <td>train</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64167</th>\n",
              "      <td>0xf5ba78</td>\n",
              "      <td>One of my favorite episodes.</td>\n",
              "      <td>[]</td>\n",
              "      <td>train</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64169</th>\n",
              "      <td>0xb5a35a</td>\n",
              "      <td>Texans and Astros both shut out tonight. Houst...</td>\n",
              "      <td>[texans, astros, sadness, losers]</td>\n",
              "      <td>train</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>47890 rows √ó 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78acd3f7-59c1-4aef-886e-a693e7b7b140')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-78acd3f7-59c1-4aef-886e-a693e7b7b140 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-78acd3f7-59c1-4aef-886e-a693e7b7b140');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-959017e3-eaf5-41f3-94ff-84523e3b283d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-959017e3-eaf5-41f3-94ff-84523e3b283d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-959017e3-eaf5-41f3-94ff-84523e3b283d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_0ad9e3c2-5312-4517-9871-003d26dc0f06\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_train')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0ad9e3c2-5312-4517-9871-003d26dc0f06 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_train');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train",
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 47890,\n  \"fields\": [\n    {\n      \"column\": \"post_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 47890,\n        \"samples\": [\n          \"0xb38039\",\n          \"0x0f204c\",\n          \"0xf99ae5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 47888,\n        \"samples\": [\n          \"Aw you seem upset, you are obsessed with talking shit about me. This move is indefensible.\",\n          \"Bruh, I\\u2019ve been getting this tough love since I was 18. Hence the dusty balls.\",\n          \"They might still be a minority but you can't argue with the fact that they have been on the rise,\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hashtags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"train\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"emotion\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"joy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emotion\n",
            "joy         0.496910\n",
            "anger       0.223303\n",
            "surprise    0.131155\n",
            "sadness     0.081980\n",
            "fear        0.041950\n",
            "disgust     0.024702\n",
            "Name: proportion, dtype: float64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Data imbalance\\nemotion\\njoy         0.496910\\nanger       0.223303\\nsurprise    0.131155\\nsadness     0.081980\\nfear        0.041950\\ndisgust     0.024702\\n\\n\\nfrom sklearn.utils import resample\\n\\ndf = df_train.copy()\\n\\ncounts = df[\"emotion\"].value_counts()\\nprint(counts)\\n\\n# ‰æãÂ¶ÇÔºöËÆìÊØè‰∏ÄÈ°ûËá≥Â∞ëÊúâ min_countÔºåÊúÄÂ§ö‰∏çË∂ÖÈÅé max_count\\nmin_count = counts.min()          # ÊúÄÂ∞ëÈ°ûÁöÑÊï∏ÈáèÔºàdisgustÔºâ\\nmax_count = int(counts.median())  # ÊàñÁî® median / Á¨¨‰∫åÂ§öÁöÑÈ°ûÁï∂‰∏äÈôê\\n\\ndfs = []\\nfor label, cnt in counts.items():\\n    df_label = df[df[\"emotion\"] == label]\\n\\n    if cnt > max_count:\\n        # Â§öÊï∏È°û ‚Üí downsample Âà∞ max_count\\n        df_label_new = resample(\\n            df_label,\\n            replace=False,\\n            n_samples=max_count,\\n            random_state=42\\n        )\\n    elif cnt < min_count * 2:\\n        # Ë∂ÖÂ∞ëÊï∏È°ûÔºàÈÅ∏ÊìáÊÄßÔºâ‚Üí oversample ‰∏ÄÈªû\\n        df_label_new = resample(\\n            df_label,\\n            replace=True,\\n            n_samples=min(min_count * 2, max_count),\\n            random_state=42\\n        )\\n    else:\\n        # ÂÖ∂‰ªñÈ°ûÁ∂≠ÊåÅÂéüÊ®£\\n        df_label_new = df_label\\n\\n    dfs.append(df_label_new)\\n\\ndf_train_balanced = pd.concat(dfs).sample(frac=1, random_state=42).reset_index(drop=True)\\nprint(df_train.shape, \"‚Üí\", df_train_balanced.shape)\\nprint(df_train_balanced[\"emotion\"].value_counts())\\n\\nemotion\\njoy         23797\\nanger       10694\\nsurprise     6281\\nsadness      3926\\nfear         2009\\ndisgust      1183\\nName: count, dtype: int64\\n(47890, 5) ‚Üí (23967, 5)\\nemotion\\njoy         5103\\nsurprise    5103\\nanger       5103\\nsadness     3926\\nfear        2366\\ndisgust     2366\\nName: count, dtype: int64\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "### Add the code related to the preprocessing steps in cells inside this section\n",
        "\n",
        "!pip install emoji\n",
        "!pip install nltk\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# load dataset\n",
        "#/content/drive/MyDrive/dm-lab-2-private-competition/data_identification.csv\n",
        "df_id = pd.read_csv(\"/content/drive/MyDrive/dm-lab-2-private-competition/data_identification.csv\")\n",
        "df_emotion = pd.read_csv(\"/content/drive/MyDrive/dm-lab-2-private-competition/emotion.csv\")\n",
        "with open('/content/drive/MyDrive/dm-lab-2-private-competition/final_posts.json','r') as file:\n",
        "        final_posts = json.load(file)\n",
        "\n",
        "rows = []\n",
        "for item in final_posts:\n",
        "    post = item[\"root\"][\"_source\"][\"post\"]\n",
        "    rows.append({\n",
        "        \"post_id\": post[\"post_id\"],\n",
        "        \"text\": post[\"text\"],\n",
        "        \"hashtags\": post.get(\"hashtags\", [])  # Ê≤íÊúâÂ∞±Áµ¶Á©∫ list\n",
        "    })\n",
        "df_posts = pd.DataFrame(rows)\n",
        "#df_posts.head()\n",
        "\n",
        "\n",
        "df_all = df_posts.merge(df_id, left_on=\"post_id\",right_on=\"id\", how=\"left\")\n",
        "df_all = df_all.drop(columns=[\"id\"])\n",
        "df_all = df_all.merge(df_emotion,left_on=\"post_id\", right_on=\"id\", how=\"left\")\n",
        "df_all = df_all.drop(columns=[\"id\"])\n",
        "#df_all.head()\n",
        "\n",
        "import re\n",
        "# text cleaning\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.strip()\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text) # remove url\n",
        "    text = re.sub(r\"[\\r\\n\\t]+\", \" \", text)  # ÊèõË°å / tab -> Á©∫ÁôΩ\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  # normalize whitespace\n",
        "    text = re.sub(r\"([!?.,])\\1+\", r\"\\1\", text)  # limit repeated punctuations\n",
        "    text = emoji.demojize(text) # transform emoji into token\n",
        "    return text\n",
        "df_all[\"text\"] = df_all[\"text\"].apply(clean_text)\n",
        "\n",
        "# hashtag processing: combine text and hashtag\n",
        "def combine_text_and_hashtags(row):\n",
        "    text = row[\"text\"]\n",
        "    tags = row[\"hashtags\"]\n",
        "    if isinstance(tags, list) and len(tags) > 0:\n",
        "        tag_str = \" \" + \" \".join(\"#\" + t for t in tags)\n",
        "    else:\n",
        "        tag_str = \"\"\n",
        "    return text + tag_str\n",
        "df_all[\"text\"] = df_all.apply(combine_text_and_hashtags, axis=1)\n",
        "\n",
        "# chceck duplicate value\n",
        "dup = df_all[df_all.duplicated(subset=[\"text\"], keep=False)]\n",
        "#display(dup)\n",
        "\n",
        "#check the null value (Data Sanity)\n",
        "print(df_all[\"text\"].isna().sum())\n",
        "\n",
        "\n",
        "# split\n",
        "df_train= df_all[df_all['split']== 'train'].dropna(subset=[\"emotion\"])\n",
        "df_test= df_all[df_all['split']== 'test'].copy()\n",
        "\n",
        "print(df_train[\"emotion\"].isna().sum())\n",
        "\n",
        "display(df_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfptT03K_sZX"
      },
      "source": [
        "## 2. Feature Engineering Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw5AcH9a_sZX",
        "outputId": "c9c2f272-553e-4431-d8c8-dcc3edc67862",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF shape: (47890, 20000) (16281, 20000)\n",
            "(47890, 50000)\n"
          ]
        }
      ],
      "source": [
        "### Add the code related to the feature engineering steps in cells inside this section\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "char_vectorizer = TfidfVectorizer(\n",
        "    strip_accents=\"unicode\",\n",
        "    analyzer=\"char\",\n",
        "    ngram_range=(3, 5),\n",
        "    min_df=3,\n",
        "    max_features=30000\n",
        ")\n",
        "\n",
        "X_train_char = char_vectorizer.fit_transform(df_train[\"text\"])\n",
        "X_test_char  = char_vectorizer.transform(df_test[\"text\"])\n",
        "\n",
        "# TF-IDF unigram + bigram ----\n",
        "# input: cleaned + hashtag ÁöÑ text\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    strip_accents=\"unicode\",\n",
        "    max_features=20000,\n",
        "    ngram_range=(1, 2),      # unigram + bigram\n",
        "    max_df=0.95,\n",
        "    min_df=3,                # Ëá≥Â∞ëÂú® 3 ÂÄãÊñá‰ª∂‰∏≠Âá∫Áèæ\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(df_train[\"text\"])\n",
        "X_test_tfidf  = tfidf_vectorizer.transform(df_test[\"text\"])\n",
        "\n",
        "print(\"TF-IDF shape:\", X_train_tfidf.shape, X_test_tfidf.shape)\n",
        "\n",
        "\n",
        "X_train_all = hstack([X_train_tfidf, X_train_char])\n",
        "X_test_all  = hstack([X_test_tfidf, X_test_char])\n",
        "\n",
        "print(X_train_all.shape)\n",
        "\n",
        "# save vectorizer\n",
        "import pickle\n",
        "\n",
        "pickle.dump(tfidf_vectorizer, open(\"tfidf_word_vectorizer.pkl\", \"wb\"))\n",
        "pickle.dump(char_vectorizer, open(\"tfidf_char_vectorizer.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vc2WGw__sZY"
      },
      "source": [
        "## 3. Model Implementation Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ej11Ujja_sZY",
        "outputId": "22c037c0-0565-4d6a-db53-11ae286c8a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 43101\n",
            "Valid size: 4789\n",
            "Labels: ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n",
            "Class weights: tensor([0.7463, 6.7451, 3.9732, 0.3354, 2.0333, 1.2707])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8082' max='8082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8082/8082 56:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.602200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.361300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.278700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.278500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.237700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.276000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.217500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.206900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.241900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.197900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>1.194500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>1.202800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>1.134700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>1.133200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.027700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>1.011800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>1.040500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>1.039500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>1.045600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.022700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>1.014400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>0.987900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>1.062800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>1.025000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>1.002900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>1.005200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>1.039000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>0.893000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5800</td>\n",
              "      <td>0.883300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.822500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6200</td>\n",
              "      <td>0.917000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6400</td>\n",
              "      <td>0.890200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>0.894700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6800</td>\n",
              "      <td>0.866500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.861700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>0.813400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7400</td>\n",
              "      <td>0.838100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7600</td>\n",
              "      <td>0.867100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>0.813100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.886000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 1.2221040725708008, 'eval_macro_f1': 0.5333114858318388, 'eval_accuracy': 0.6393819168928795, 'eval_runtime': 31.7501, 'eval_samples_per_second': 150.834, 'eval_steps_per_second': 9.449, 'epoch': 3.0}\n",
            "Validation macro F1: 0.5333114858318388\n",
            "Validation accuracy: 0.6393819168928795\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.62      0.56      0.59      1069\n",
            "     disgust       0.23      0.41      0.29       118\n",
            "        fear       0.44      0.73      0.55       201\n",
            "         joy       0.87      0.70      0.78      2380\n",
            "     sadness       0.39      0.54      0.45       393\n",
            "    surprise       0.48      0.63      0.54       628\n",
            "\n",
            "    accuracy                           0.64      4789\n",
            "   macro avg       0.50      0.59      0.53      4789\n",
            "weighted avg       0.69      0.64      0.66      4789\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission_roberta_colab.csv\n"
          ]
        }
      ],
      "source": [
        "### Add the code related to the model implementation steps in cells inside this section\n",
        "\n",
        "!pip install -q -U transformers accelerate\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "# train/valid split\n",
        "\n",
        "train_df, valid_df = train_test_split(\n",
        "    df_train[[\"text\", \"emotion\"]],\n",
        "    test_size=0.1,\n",
        "    random_state=42,\n",
        "    stratify=df_train[\"emotion\"]\n",
        ")\n",
        "\n",
        "labels = sorted(df_train[\"emotion\"].unique())\n",
        "label2id = {lbl: i for i, lbl in enumerate(labels)}\n",
        "id2label = {i: lbl for lbl, i in label2id.items()}\n",
        "num_labels = len(labels)\n",
        "\n",
        "train_df = train_df.copy()\n",
        "valid_df = valid_df.copy()\n",
        "train_df[\"label\"] = train_df[\"emotion\"].map(label2id)\n",
        "valid_df[\"label\"] = valid_df[\"emotion\"].map(label2id)\n",
        "\n",
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Valid size:\", len(valid_df))\n",
        "print(\"Labels:\", labels)\n",
        "\n",
        "# class weightsÔºàhandling data imblanceÔºâ\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.arange(num_labels),\n",
        "    y=train_df[\"label\"].values\n",
        ")\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# tokenizer\n",
        "\n",
        "model_name = \"roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Ëá™Ë®Ç PyTorch DatasetÔºàEmotionDataset)\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length=256, has_labels=True):\n",
        "        self.texts = df[\"text\"].tolist()\n",
        "        self.has_labels = has_labels\n",
        "        if has_labels:\n",
        "            self.labels = df[\"label\"].tolist()\n",
        "        else:\n",
        "            self.labels = None\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        item = {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "        }\n",
        "        if self.has_labels:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "train_dataset = EmotionDataset(train_df, tokenizer, max_length=256, has_labels=True)\n",
        "valid_dataset = EmotionDataset(valid_df, tokenizer, max_length=256, has_labels=True)\n",
        "\n",
        "# modelÔºàRoBERTa-baseÔºâ+ weighted loss\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, class_weights, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "    def compute_loss(\n",
        "        self,\n",
        "        model,\n",
        "        inputs,\n",
        "        return_outputs: bool = False,\n",
        "        num_items_in_batch: int | None = None,\n",
        "    ):\n",
        "        # ÊãølabelsÔºànot change the structure of inputs too muchÔºâ\n",
        "        labels = inputs[\"labels\"]\n",
        "\n",
        "        # put in modelÔºàËá™ÂãïÁî® input_ids / attention_maskÔºâ\n",
        "        outputs = model(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "        )\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # class weights\n",
        "        cw = self.class_weights.to(logits.device)\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=cw)\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# 6. TrainingArgumentsÔºà Colab GPU Ôºâ\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./roberta_emotion\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=200,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"macro_f1\": macro_f1, \"accuracy\": acc}\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    class_weights=class_weights,\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "#  training\n",
        "\n",
        "trainer.train()\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n",
        "\n",
        "# valid set\n",
        "\n",
        "valid_logits = trainer.predict(valid_dataset).predictions\n",
        "valid_preds = np.argmax(valid_logits, axis=-1)\n",
        "\n",
        "print(\"Validation macro F1:\", f1_score(valid_df[\"label\"], valid_preds, average=\"macro\"))\n",
        "print(\"Validation accuracy:\", accuracy_score(valid_df[\"label\"], valid_preds))\n",
        "print(\n",
        "    classification_report(\n",
        "        valid_df[\"label\"],\n",
        "        valid_preds,\n",
        "        target_names=labels\n",
        "    )\n",
        ")\n",
        "\n",
        "# test prediction + submission\n",
        "\n",
        "test_df = df_test.copy()[[\"text\", \"post_id\"]]\n",
        "test_dataset = EmotionDataset(test_df, tokenizer, max_length=128, has_labels=False)\n",
        "\n",
        "test_logits = trainer.predict(test_dataset).predictions\n",
        "test_labels = np.argmax(test_logits, axis=-1)\n",
        "test_emotions = [id2label[i] for i in test_labels]\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test_df[\"post_id\"],\n",
        "    \"emotion\": test_emotions\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission_roberta_colab.csv\", index=False)\n",
        "print(\"Saved submission_roberta_colab.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JSAvkvX43igw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}